{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcA_Rod3kaDa"
      },
      "source": [
        "#Hadamard and concatation Mearged Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dh47Ttn1ZH_3"
      },
      "source": [
        "#Build A Multi-Model\n",
        "\n",
        "For this section, pretrained unimodels are required as well as some files which were created during that process.\n",
        "\n",
        "To merge the models we require the output at dense layer of 300 neurons from unimodels. In the case of audio and transcript it requires some pre-processing before passing through the model, and that preprocessing requires to have all the file loaded first, so to avoid that process here some files were generated during the unimodel building process. So those files will be used to get the inputs of audio and transcript model.\n",
        "\n",
        "While in video a sigle video file can be pre-processed and that processe implemented here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_39EvQdv32nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5f9d515-fd18-4586-ecdc-5d7f6dcd05de"
      },
      "source": [
        "import os   # To perform directory related operations\n",
        "import cv2  # To preprocess video \n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.compat.v1.disable_v2_behavior()\n",
        "import numpy as np  # Datatype to store data\n",
        "from tensorflow.keras import layers, Model, Input,callbacks,regularizers,models  # To build 3d CNN ML model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehq0vYf9l-sY",
        "outputId": "a80cf66c-f9e9-45f4-bf02-6ae46109f65b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ONhbHfEZdT8"
      },
      "source": [
        "### Functions and Class for processing all the files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_z2Aho7fZhFv"
      },
      "source": [
        "### Class to get 300d vector of video file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Re8mGsIZbrY"
      },
      "source": [
        "class video_featur_ext:\n",
        "  \"\"\" This class is to convert any given video file to a 300d feature vector.\n",
        "  This class store a trained model, input video is processed using \n",
        "  video_to_array function\"\"\"\n",
        "\n",
        "  def __init__(self,path_to_model):\n",
        "    \"\"\" It requires to have pre trained model of video and \n",
        "    function of video to\"\"\"\n",
        "\n",
        "    self.model = models.load_model(path_to_video_model)\n",
        "    self.inter_model = None\n",
        "    # Find the layer that having output of 300d\n",
        "    for i in range(len(self.model.layers)):\n",
        "      output_layer = self.model.get_layer(index=i).output\n",
        "      if output_layer.shape.as_list() == [None,300]:\n",
        "        self.inter_model = models.Model(inputs = self.model.input,outputs = output_layer)\n",
        "        break\n",
        "\n",
        "    _,frames,width,height,channels = self.model.input_shape\n",
        "    self.frames = frames\n",
        "    self.width = width\n",
        "    self.height = height\n",
        "    self.channels = channels\n",
        "\n",
        "  def get_vector(self,v_file):\n",
        "    \n",
        "    tmp_video_array = self.pre_processing_fn(v_file,(self.width,self.height),self.frames)\n",
        "    \n",
        "    tmp_video_array = tmp_video_array.reshape(1,self.frames,self.width,self.height,self.channels)\n",
        "    return self.inter_model.predict(tmp_video_array)\n",
        "\n",
        "  def pre_processing_fn(self,files,dia,n):\n",
        "    \"\"\" Files is the path to the file and dia is output dimention and\n",
        "    n is the number of frames required\n",
        "    \"\"\"\n",
        "    cap=cv2.VideoCapture(files)  # Read the File\n",
        "    total_frames=cap.get(cv2.CAP_PROP_FRAME_COUNT) # Get Total Frames\n",
        "    sample_at=int(total_frames/(n+1))  # k/n frame\n",
        "    sample_at = sample_at if sample_at != 0 else 1\n",
        "    cap.set(1,sample_at)  # First first k/n frame\n",
        "\n",
        "    ret,frame=cap.read()  # Store that Frame \n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
        "    frame= cv2.resize(frame,dia)  # resize frame\n",
        "    frame= frame.reshape((1,dia[0],dia[1],3))  \n",
        "    \n",
        "    for i in range(1,n):\n",
        "        # Read remaning k/n th frames\n",
        "#         print(i)\n",
        "        s_at=sample_at*(i+1) \n",
        "        cap.set(1,s_at)\n",
        "        ret,frm=cap.read()\n",
        "        if not ret:\n",
        "          # print(\"Break:\",ret)\n",
        "          break\n",
        "        frm = cv2.cvtColor(frm, cv2.COLOR_BGR2RGB)\n",
        "        frm= cv2.resize(frm,dia)\n",
        "        frm= frm.reshape((1,dia[0],dia[1],3))\n",
        "        frame=np.concatenate([frame,frm], axis=0)\n",
        "\n",
        "    if frame.shape[0] < n:\n",
        "        en = frame.shape[0]\n",
        "        d = n - en\n",
        "        frame = np.concatenate([frame,np.zeros((d,dia[0],dia[1],3))],axis=0)\n",
        "    \n",
        "    return frame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdh9E8awZtgB"
      },
      "source": [
        "### Class to get 300d vector of Transcript"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A2l8WvFZrmX"
      },
      "source": [
        "class Text_feature_ext:\n",
        "  \"\"\" This class is implemented to convert the given transcripts to the 300d vector,\n",
        "  However, this class does not takes transcript file as input but only it's name.\n",
        "  it uses the text file which was created earlier in unimodel named 'textTo100dV.txt' to get\n",
        "  100d vector of sentence and 'conversionTxt100x300V.txt to get 100x300 matrix \n",
        "  representation of perticular trial's transcrip \"\"\"\n",
        "\n",
        "  def __init__(self,trained_text_model,text_to_100d_file,conversion_matrix_file):\n",
        "    \"\"\" In this constructor it reuire a pretrained model with atleast one dense layer with\n",
        "    300 neuron, a text files which were created using unimodel code.\"\"\"\n",
        "    self.trained_text_model = models.load_model(trained_text_model)  # Load pre-trained model\n",
        "    self.text_to_100d_file = {}  # This will keep map of trial name to 100d vector\n",
        "    # To read text file to create above map\n",
        "    with open(text_to_100d_file,encoding=\"utf-8\") as tvfile:\n",
        "      for line in tvfile:\n",
        "        file_name, vector = line.split(\" \",maxsplit=1)\n",
        "        vector = np.fromstring(vector, \"i\", sep=\" \")\n",
        "        self.text_to_100d_file[file_name] = vector\n",
        "\n",
        "    # To read the conversion matrix text file    \n",
        "    self.conversion_matrix_file = []\n",
        "    with open(conversion_matrix_file,encoding=\"utf-8\") as cmfile:\n",
        "      for line in cmfile:\n",
        "        vector = np.fromstring(line, \"f\", sep=\" \")\n",
        "        self.conversion_matrix_file.append(vector)\n",
        "    self.conversion_matrix_file = np.array(self.conversion_matrix_file)\n",
        "\n",
        "    # Creating a sub/inter - model of pretrained model to get output from that \n",
        "    # Dense layer with 300 neurons\n",
        "    self.inter_model = None\n",
        "    # Find the layer that having output of 300d\n",
        "    for i in range(len(self.trained_text_model.layers)):\n",
        "      output_layer = self.trained_text_model.get_layer(index=i).output\n",
        "      if output_layer.shape.as_list() == [None,300]:\n",
        "        self.inter_model = models.Model(inputs = self.trained_text_model.input,outputs = output_layer)\n",
        "        break\n",
        "    \n",
        "  def get_vector(self,t_file_name):\n",
        "    \"\"\" This will reqire just th file name without path. \n",
        "    first this function loads the 100d vector of perticular trial then,\n",
        "    converts to 100x300 sized matrix using conversion matrix.\n",
        "    that marix is given to that inter-model. The output of that model\n",
        "    is returned\"\"\"\n",
        "    text_100dV = self.text_to_100d_file[t_file_name]\n",
        "    cm_input = self.conversion_matrix_file[text_100dV].T\n",
        "    cm_input = cm_input.reshape(1,cm_input.shape[0],cm_input.shape[1])\n",
        "    return self.inter_model.predict(cm_input)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4ndv29Had5i"
      },
      "source": [
        "### Class to get 39 Micro Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Os1v3Q3aflK"
      },
      "source": [
        "class annotation_ext:\n",
        "  \"\"\" This is simple class to read the micro feature csv\"\"\"\n",
        "  def __init__(self,anotation_file_path):\n",
        "    self.micro_expression_vector = {}\n",
        "    self.micro_expression_label = {}\n",
        "    with open(anotation_file_path,encoding='utf-8') as anfile:\n",
        "      next(anfile) # Skipp the first row\n",
        "      for line in anfile:\n",
        "        file_name = line.split(\",\")[0]\n",
        "        label = line.split(\",\")[-1]\n",
        "        vector39F = np.fromstring(\" \".join(line.split(\",\")[1:-1]),\"i\",sep=\" \")\n",
        "        self.micro_expression_vector[file_name] = vector39F.reshape(1,vector39F.shape[0])\n",
        "        self.micro_expression_label[file_name] = label[:-1]\n",
        "\n",
        "  def get_vector(self,file_name):\n",
        "    \"\"\" When File name is provided it returns the 39d vector\"\"\"\n",
        "    return self.micro_expression_vector[file_name]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYWgM-IVai7p"
      },
      "source": [
        "### Class to get 300d vector of audio file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydk_pWgUalnu"
      },
      "source": [
        "class audio_feature_ext:\n",
        "  \"\"\" This class will reqire a matrix of preprocessed audio features. and a \n",
        "  trained model of audio classification get_vector function will return 300 vector,\n",
        "  A Train model should have a layer with 300 neurons \"\"\"\n",
        "  def __init__(self,path_to_trained_model,feature_matrix_text,truth_lie_list):\n",
        "    self.model = models.load_model(path_to_trained_model)\n",
        "    self.inter_model = None\n",
        "\n",
        "    # Find the layer that having output of 300d\n",
        "    for i in range(len(self.model.layers)):\n",
        "      output_layer = self.model.get_layer(index=i).output\n",
        "      if output_layer.shape.as_list() == [None,300]:\n",
        "        self.inter_model = models.Model(inputs = self.model.input,outputs = output_layer)\n",
        "        break\n",
        "    self.audio_to_feat = {}\n",
        "    \n",
        "    with open(feature_matrix_text,encoding=\"utf-8\") as affile:\n",
        "      i = 0\n",
        "      for line in affile:\n",
        "        vector = np.fromstring(line, \"f\", sep=\",\")\n",
        "        vector = vector[:-1]\n",
        "        self.audio_to_feat[truth_lie_list[i]] = vector.reshape(1,vector.shape[0])\n",
        "        i += 1\n",
        "  def get_vector(self,file_name):\n",
        "    tx_vector = self.audio_to_feat[file_name]\n",
        "\n",
        "    return self.inter_model.predict(tx_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6xyTjn4Lt2P"
      },
      "source": [
        "### Load Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUWNu6TpaqaB"
      },
      "source": [
        "# Define Some directory\n",
        "path_to_truth_video = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Clips/Truthful\"\n",
        "path_to_lie_video = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Clips/Deceptive\"\n",
        "\n",
        "path_to_video_model = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Clips/VideoModel\"\n",
        "path_to_audio_model = \"/content/drive/MyDrive/RealLife_DeceptionDetection/audio/Best_Audio_1\"\n",
        "path_to_text_model  = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Transcription/Transcript1DCNNModel\"\n",
        "\n",
        "path_to_text100d_file = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Transcription/textTo100dV.txt\"\n",
        "path_to_text100x300cm = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Transcription/conversionTxt100x300V.txt\"\n",
        "path_to_csv_micro_exp = \"/content/drive/MyDrive/RealLife_DeceptionDetection/Annotation/All_Gestures_Deceptive and Truthful.csv\"\n",
        "\n",
        "path_to_audio_features = \"/content/drive/MyDrive/RealLife_DeceptionDetection/audio/audio_feature.csv\"\n",
        "\n",
        "truth_vfiles = sorted(os.listdir(path_to_truth_video))\n",
        "lie_vfiles = sorted(os.listdir(path_to_lie_video))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3MiomE8aubn"
      },
      "source": [
        "# Create 300d Vector generator objects\n",
        "\n",
        "# Audio\n",
        "audio_file_names = [ afiles[:-4]+\".wav\" for afiles in truth_vfiles + lie_vfiles]\n",
        "audio_300dGen = audio_feature_ext(path_to_audio_model,path_to_audio_features,audio_file_names) \n",
        "\n",
        "# Video\n",
        "video_300dGen = video_featur_ext(path_to_video_model)\n",
        "\n",
        "# Transcript \n",
        "Text_300dGen = Text_feature_ext(path_to_text_model,path_to_text100d_file,path_to_text100x300cm)\n",
        "\n",
        "# Annotation\n",
        "Annotation_39dGen = annotation_ext(path_to_csv_micro_exp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af0f5LlaLzVa"
      },
      "source": [
        "### Store 300-d Vectors from each model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdI6q5RtayXy"
      },
      "source": [
        "# First Create 339d Vector \n",
        "\n",
        "truth_339d = []\n",
        "lie_339d = []\n",
        "\n",
        "for id in truth_vfiles:\n",
        "  v_file_path = os.path.join(path_to_truth_video,id)\n",
        "  t_file_name = id[:-4]+\".txt\"\n",
        "  a_file_name = id[:-4]+\".wav\"\n",
        "  vector_1 = video_300dGen.get_vector(v_file_path)\n",
        "  vector_2 = Text_300dGen.get_vector(t_file_name)\n",
        "  vector_3 = audio_300dGen.get_vector(a_file_name)\n",
        "  vector_4 = Annotation_39dGen.get_vector(id)\n",
        "  vector_h = vector_1 * vector_2 * vector_3\n",
        "  vector_h_c= np.concatenate([vector_h,vector_4], axis=1)\n",
        "  \n",
        "  truth_339d.append(vector_h_c)\n",
        "truth_339d = np.array(truth_339d)\n",
        "\n",
        "\n",
        "for id in lie_vfiles:\n",
        "  v_file_path = os.path.join(path_to_lie_video,id)\n",
        "  t_file_name = id[:-4]+\".txt\"\n",
        "  a_file_name = id[:-4]+\".wav\"\n",
        "  vector_1 = video_300dGen.get_vector(v_file_path)\n",
        "  vector_2 = Text_300dGen.get_vector(t_file_name)\n",
        "  vector_3 = audio_300dGen.get_vector(a_file_name)\n",
        "  vector_4 = Annotation_39dGen.get_vector(id)\n",
        "  vector_h = vector_1 * vector_2 * vector_3\n",
        "  vector_h_c= np.concatenate([vector_h,vector_4], axis=1)\n",
        "  \n",
        "  lie_339d.append(vector_h_c)\n",
        "lie_339d = np.array(lie_339d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAewbt-koFzU",
        "outputId": "bffe67ac-03b3-4b03-a8d0-cd94b44d3a76"
      },
      "source": [
        "lie_339d.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61, 1, 339)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq0rXtEbjFDN"
      },
      "source": [
        "truth_339d = truth_339d.reshape(truth_339d.shape[0],truth_339d.shape[2])\n",
        "lie_339d = lie_339d.reshape(lie_339d.shape[0],lie_339d.shape[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWl_bAubL7vX"
      },
      "source": [
        "### Split Train and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyPdtjSijY9b"
      },
      "source": [
        "n = int(truth_339d.shape[0]*0.7)\n",
        "x_train_truth = truth_339d[:n]\n",
        "y_train_truth = np.ones(shape = (x_train_truth.shape[0],1))\n",
        "\n",
        "x_train_lie = lie_339d[:n]\n",
        "y_train_lie = np.zeros(shape = (x_train_lie.shape[0],1))\n",
        "\n",
        "x_train = np.concatenate([x_train_truth,x_train_lie],axis=0)\n",
        "y_train = np.concatenate([y_train_truth,y_train_lie],axis=0)\n",
        "\n",
        "\n",
        "x_test_truth = truth_339d[n:]\n",
        "y_test_truth = np.ones(shape = (x_test_truth.shape[0],1))\n",
        "\n",
        "x_test_lie = lie_339d[n:]\n",
        "y_test_lie = np.zeros(shape = (x_test_lie.shape[0],1))\n",
        "\n",
        "x_test = np.concatenate([x_test_truth,x_test_lie],axis=0)\n",
        "y_test = np.concatenate([y_test_truth,y_test_lie],axis=0)\n",
        "\n",
        "# Shuffle Training data\n",
        "sd = 1112\n",
        "rnd = np.random.RandomState(sd)\n",
        "rnd.shuffle(y_train)\n",
        "\n",
        "sd = 1112\n",
        "rnd = np.random.RandomState(sd)\n",
        "rnd.shuffle(x_train)\n",
        "\n",
        "# Shuffle Testing data\n",
        "sd = 1114\n",
        "rnd = np.random.RandomState(sd)\n",
        "rnd.shuffle(y_test)\n",
        "\n",
        "sd = 1114\n",
        "rnd = np.random.RandomState(sd)\n",
        "rnd.shuffle(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_auRY9SJMAe4"
      },
      "source": [
        "### Build MLP model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB1gxlKzjuzF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66e4af8-fb91-44e1-d02b-6ef932832220"
      },
      "source": [
        "# Following \"cb\" Function is callback function to early stop the training based on validation set accuracy\n",
        "cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\",\n",
        "    min_delta=0.01,\n",
        "    patience=200,\n",
        "    verbose=0,\n",
        "    mode=\"max\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "\n",
        "# Following is the 1D CNN for finding features from the text word2vectors\n",
        "# Model is build using daisy chain layers;\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(x_train.shape[1]))\n",
        "\n",
        "x = layers.Dense(1024,activation=\"relu\")(int_sequences_input)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "preds = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "# Combine all the layers into a model\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 339)]             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              348160    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 350,210\n",
            "Trainable params: 350,210\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCJGYWhDMGNX"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmpbWWsZkBBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ec46e9a-8085-4e8b-c50c-f561c6d46a01"
      },
      "source": [
        "# Model is compiled and loss is considered as sparse Categorical Cross Entropy\n",
        "# To calculate model weights RMSProp is used\n",
        "\n",
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer='rmsprop', metrics=[\"acc\"]\n",
        ")\n",
        "\n",
        "\"\"\" Model is trained using training data; Validation data is used to check the model's accuracy on test data\n",
        "    Note:- these validation data is not used in weight update\"\"\" \n",
        "history = model.fit(x_train, y_train,epochs=1000,validation_data=(x_test, y_test),callbacks=[cb])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 84 samples, validate on 37 samples\n",
            "Epoch 1/1000\n",
            "84/84 [==============================] - 0s 3ms/sample - loss: 27.2244 - acc: 0.5000 - val_loss: 2.7181 - val_acc: 0.4054\n",
            "Epoch 2/1000\n",
            "84/84 [==============================] - 0s 201us/sample - loss: 20.8272 - acc: 0.6310 - val_loss: 0.9985 - val_acc: 0.5946\n",
            "Epoch 3/1000\n",
            "84/84 [==============================] - 0s 154us/sample - loss: 6.1904 - acc: 0.8095 - val_loss: 8.2844 - val_acc: 0.5405\n",
            "Epoch 4/1000\n",
            "84/84 [==============================] - 0s 156us/sample - loss: 4.6734 - acc: 0.8690 - val_loss: 1.4844 - val_acc: 0.5135\n",
            "Epoch 5/1000\n",
            "84/84 [==============================] - 0s 199us/sample - loss: 12.7972 - acc: 0.7857 - val_loss: 6.9712 - val_acc: 0.5676\n",
            "Epoch 6/1000\n",
            "84/84 [==============================] - 0s 155us/sample - loss: 6.2454 - acc: 0.8571 - val_loss: 4.8129 - val_acc: 0.5405\n",
            "Epoch 7/1000\n",
            "84/84 [==============================] - 0s 150us/sample - loss: 3.2557 - acc: 0.8810 - val_loss: 2.5822 - val_acc: 0.5676\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 8/1000\n",
            "84/84 [==============================] - 0s 156us/sample - loss: 1.5948 - acc: 0.9286 - val_loss: 3.5775 - val_acc: 0.5946\n",
            "Epoch 9/1000\n",
            "84/84 [==============================] - 0s 155us/sample - loss: 2.2764 - acc: 0.9286 - val_loss: 1.9031 - val_acc: 0.6757\n",
            "Epoch 10/1000\n",
            "84/84 [==============================] - 0s 152us/sample - loss: 10.3686 - acc: 0.8333 - val_loss: 2.2314 - val_acc: 0.7297\n",
            "Epoch 11/1000\n",
            "84/84 [==============================] - 0s 157us/sample - loss: 6.9313 - acc: 0.8810 - val_loss: 3.6264 - val_acc: 0.5676\n",
            "Epoch 12/1000\n",
            "84/84 [==============================] - 0s 143us/sample - loss: 1.7945 - acc: 0.9048 - val_loss: 1.0645 - val_acc: 0.7027\n",
            "Epoch 13/1000\n",
            "84/84 [==============================] - 0s 135us/sample - loss: 0.8127 - acc: 0.9286 - val_loss: 4.5874 - val_acc: 0.5946\n",
            "Epoch 14/1000\n",
            "84/84 [==============================] - 0s 159us/sample - loss: 2.0849 - acc: 0.8929 - val_loss: 3.6635 - val_acc: 0.6216\n",
            "Epoch 15/1000\n",
            "84/84 [==============================] - 0s 175us/sample - loss: 1.4519 - acc: 0.9048 - val_loss: 1.1422 - val_acc: 0.7027\n",
            "Epoch 16/1000\n",
            "84/84 [==============================] - 0s 170us/sample - loss: 2.2452 - acc: 0.8452 - val_loss: 1.0916 - val_acc: 0.7297\n",
            "Epoch 17/1000\n",
            "84/84 [==============================] - 0s 181us/sample - loss: 9.6650 - acc: 0.9048 - val_loss: 3.3103 - val_acc: 0.6757\n",
            "Epoch 18/1000\n",
            "84/84 [==============================] - 0s 218us/sample - loss: 0.3994 - acc: 0.9167 - val_loss: 3.5227 - val_acc: 0.6486\n",
            "Epoch 19/1000\n",
            "84/84 [==============================] - 0s 195us/sample - loss: 0.8701 - acc: 0.9524 - val_loss: 4.8765 - val_acc: 0.6216\n",
            "Epoch 20/1000\n",
            "84/84 [==============================] - 0s 151us/sample - loss: 2.7360 - acc: 0.9167 - val_loss: 6.5095 - val_acc: 0.6216\n",
            "Epoch 21/1000\n",
            "84/84 [==============================] - 0s 162us/sample - loss: 0.0460 - acc: 0.9881 - val_loss: 5.5680 - val_acc: 0.6216\n",
            "Epoch 22/1000\n",
            "84/84 [==============================] - 0s 144us/sample - loss: 1.6094 - acc: 0.9286 - val_loss: 0.8675 - val_acc: 0.7297\n",
            "Epoch 23/1000\n",
            "84/84 [==============================] - 0s 148us/sample - loss: 2.8741 - acc: 0.9524 - val_loss: 3.3016 - val_acc: 0.6216\n",
            "Epoch 24/1000\n",
            "84/84 [==============================] - 0s 161us/sample - loss: 0.3174 - acc: 0.9286 - val_loss: 13.2746 - val_acc: 0.6216\n",
            "Epoch 25/1000\n",
            "84/84 [==============================] - 0s 166us/sample - loss: 0.8338 - acc: 0.9643 - val_loss: 10.6093 - val_acc: 0.5946\n",
            "Epoch 26/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 5.2559 - acc: 0.9762 - val_loss: 5.3911 - val_acc: 0.5676\n",
            "Epoch 27/1000\n",
            "84/84 [==============================] - 0s 216us/sample - loss: 6.9525 - acc: 0.9405 - val_loss: 3.3384 - val_acc: 0.5946\n",
            "Epoch 28/1000\n",
            "84/84 [==============================] - 0s 161us/sample - loss: 0.5066 - acc: 0.9643 - val_loss: 5.6450 - val_acc: 0.6216\n",
            "Epoch 29/1000\n",
            "84/84 [==============================] - 0s 159us/sample - loss: 1.0927 - acc: 0.9286 - val_loss: 12.6301 - val_acc: 0.6216\n",
            "Epoch 30/1000\n",
            "84/84 [==============================] - 0s 185us/sample - loss: 0.0970 - acc: 0.9762 - val_loss: 4.7385 - val_acc: 0.6216\n",
            "Epoch 31/1000\n",
            "84/84 [==============================] - 0s 215us/sample - loss: 0.1211 - acc: 0.9762 - val_loss: 3.8441 - val_acc: 0.6486\n",
            "Epoch 32/1000\n",
            "84/84 [==============================] - 0s 201us/sample - loss: 2.7699 - acc: 0.9762 - val_loss: 3.2482 - val_acc: 0.6216\n",
            "Epoch 33/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 0.8043 - acc: 0.9643 - val_loss: 1.1006 - val_acc: 0.7027\n",
            "Epoch 34/1000\n",
            "84/84 [==============================] - 0s 209us/sample - loss: 1.2625 - acc: 0.9643 - val_loss: 6.5002 - val_acc: 0.5946\n",
            "Epoch 35/1000\n",
            "84/84 [==============================] - 0s 171us/sample - loss: 0.3395 - acc: 0.9643 - val_loss: 4.6584 - val_acc: 0.5676\n",
            "Epoch 36/1000\n",
            "84/84 [==============================] - 0s 169us/sample - loss: 3.1254 - acc: 0.9762 - val_loss: 11.7348 - val_acc: 0.5946\n",
            "Epoch 37/1000\n",
            "84/84 [==============================] - 0s 162us/sample - loss: 4.0187 - acc: 0.9524 - val_loss: 4.5932 - val_acc: 0.5946\n",
            "Epoch 38/1000\n",
            "84/84 [==============================] - 0s 150us/sample - loss: 2.2353 - acc: 0.9881 - val_loss: 2.9810 - val_acc: 0.6486\n",
            "Epoch 39/1000\n",
            "84/84 [==============================] - 0s 175us/sample - loss: 0.4405 - acc: 0.9881 - val_loss: 4.7310 - val_acc: 0.6216\n",
            "Epoch 40/1000\n",
            "84/84 [==============================] - 0s 154us/sample - loss: 0.0130 - acc: 1.0000 - val_loss: 4.7644 - val_acc: 0.6216\n",
            "Epoch 41/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 0.0075 - acc: 1.0000 - val_loss: 4.7707 - val_acc: 0.6216\n",
            "Epoch 42/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 0.1220 - acc: 0.9643 - val_loss: 7.0045 - val_acc: 0.6216\n",
            "Epoch 43/1000\n",
            "84/84 [==============================] - 0s 207us/sample - loss: 0.0067 - acc: 1.0000 - val_loss: 6.9821 - val_acc: 0.6216\n",
            "Epoch 44/1000\n",
            "84/84 [==============================] - 0s 187us/sample - loss: 2.6900 - acc: 0.9524 - val_loss: 7.3010 - val_acc: 0.6216\n",
            "Epoch 45/1000\n",
            "84/84 [==============================] - 0s 216us/sample - loss: 2.6833 - acc: 0.9881 - val_loss: 1.2155 - val_acc: 0.7838\n",
            "Epoch 46/1000\n",
            "84/84 [==============================] - 0s 179us/sample - loss: 1.8325 - acc: 0.9405 - val_loss: 4.9382 - val_acc: 0.6216\n",
            "Epoch 47/1000\n",
            "84/84 [==============================] - 0s 174us/sample - loss: 0.0074 - acc: 1.0000 - val_loss: 4.8977 - val_acc: 0.6216\n",
            "Epoch 48/1000\n",
            "84/84 [==============================] - 0s 197us/sample - loss: 1.5042 - acc: 0.9524 - val_loss: 2.5612 - val_acc: 0.6216\n",
            "Epoch 49/1000\n",
            "84/84 [==============================] - 0s 267us/sample - loss: 0.0481 - acc: 0.9881 - val_loss: 3.2339 - val_acc: 0.6486\n",
            "Epoch 50/1000\n",
            "84/84 [==============================] - 0s 201us/sample - loss: 0.2279 - acc: 0.9881 - val_loss: 5.0882 - val_acc: 0.6216\n",
            "Epoch 51/1000\n",
            "84/84 [==============================] - 0s 168us/sample - loss: 0.0421 - acc: 0.9643 - val_loss: 10.9697 - val_acc: 0.5676\n",
            "Epoch 52/1000\n",
            "84/84 [==============================] - 0s 212us/sample - loss: 1.3639 - acc: 0.9762 - val_loss: 6.5409 - val_acc: 0.6216\n",
            "Epoch 53/1000\n",
            "84/84 [==============================] - 0s 154us/sample - loss: 2.2719 - acc: 0.9643 - val_loss: 6.9705 - val_acc: 0.6486\n",
            "Epoch 54/1000\n",
            "84/84 [==============================] - 0s 216us/sample - loss: 0.0338 - acc: 0.9881 - val_loss: 7.9726 - val_acc: 0.6486\n",
            "Epoch 55/1000\n",
            "84/84 [==============================] - 0s 171us/sample - loss: 0.3304 - acc: 0.9881 - val_loss: 4.8325 - val_acc: 0.6757\n",
            "Epoch 56/1000\n",
            "84/84 [==============================] - 0s 185us/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 4.6631 - val_acc: 0.6757\n",
            "Epoch 57/1000\n",
            "84/84 [==============================] - 0s 159us/sample - loss: 1.5562 - acc: 0.9762 - val_loss: 2.0876 - val_acc: 0.6757\n",
            "Epoch 58/1000\n",
            "84/84 [==============================] - 0s 209us/sample - loss: 0.2800 - acc: 0.9643 - val_loss: 1.5331 - val_acc: 0.7297\n",
            "Epoch 59/1000\n",
            "84/84 [==============================] - 0s 160us/sample - loss: 0.2993 - acc: 0.9762 - val_loss: 4.6020 - val_acc: 0.6757\n",
            "Epoch 60/1000\n",
            "84/84 [==============================] - 0s 228us/sample - loss: 2.5736 - acc: 0.9881 - val_loss: 11.3102 - val_acc: 0.5946\n",
            "Epoch 61/1000\n",
            "84/84 [==============================] - 0s 153us/sample - loss: 3.8568 - acc: 0.9762 - val_loss: 11.7280 - val_acc: 0.5946\n",
            "Epoch 62/1000\n",
            "84/84 [==============================] - 0s 168us/sample - loss: 0.0966 - acc: 0.9881 - val_loss: 9.5644 - val_acc: 0.6216\n",
            "Epoch 63/1000\n",
            "84/84 [==============================] - 0s 164us/sample - loss: 0.0020 - acc: 1.0000 - val_loss: 9.5263 - val_acc: 0.6216\n",
            "Epoch 64/1000\n",
            "84/84 [==============================] - 0s 159us/sample - loss: 0.0191 - acc: 0.9881 - val_loss: 9.8061 - val_acc: 0.6216\n",
            "Epoch 65/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 9.7425 - val_acc: 0.6216\n",
            "Epoch 66/1000\n",
            "84/84 [==============================] - 0s 200us/sample - loss: 11.9399 - acc: 0.9405 - val_loss: 1.5006 - val_acc: 0.7027\n",
            "Epoch 67/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 0.0488 - acc: 0.9762 - val_loss: 8.4399 - val_acc: 0.6216\n",
            "Epoch 68/1000\n",
            "84/84 [==============================] - 0s 216us/sample - loss: 0.0021 - acc: 1.0000 - val_loss: 8.4241 - val_acc: 0.6216\n",
            "Epoch 69/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 0.7882 - acc: 0.9881 - val_loss: 6.0752 - val_acc: 0.6216\n",
            "Epoch 70/1000\n",
            "84/84 [==============================] - 0s 226us/sample - loss: 0.0036 - acc: 1.0000 - val_loss: 6.0092 - val_acc: 0.6216\n",
            "Epoch 71/1000\n",
            "84/84 [==============================] - 0s 230us/sample - loss: 7.3549e-04 - acc: 1.0000 - val_loss: 6.0090 - val_acc: 0.6216\n",
            "Epoch 72/1000\n",
            "84/84 [==============================] - 0s 181us/sample - loss: 0.0787 - acc: 0.9762 - val_loss: 3.8141 - val_acc: 0.7027\n",
            "Epoch 73/1000\n",
            "84/84 [==============================] - 0s 189us/sample - loss: 0.0335 - acc: 0.9881 - val_loss: 4.2489 - val_acc: 0.7027\n",
            "Epoch 74/1000\n",
            "84/84 [==============================] - 0s 182us/sample - loss: 8.0507e-04 - acc: 1.0000 - val_loss: 4.2429 - val_acc: 0.7027\n",
            "Epoch 75/1000\n",
            "84/84 [==============================] - 0s 190us/sample - loss: 0.2284 - acc: 0.9762 - val_loss: 4.4677 - val_acc: 0.7027\n",
            "Epoch 76/1000\n",
            "84/84 [==============================] - 0s 193us/sample - loss: 5.9840e-04 - acc: 1.0000 - val_loss: 4.4727 - val_acc: 0.7027\n",
            "Epoch 77/1000\n",
            "84/84 [==============================] - 0s 190us/sample - loss: 0.0285 - acc: 0.9762 - val_loss: 11.0865 - val_acc: 0.5946\n",
            "Epoch 78/1000\n",
            "84/84 [==============================] - 0s 187us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 11.0562 - val_acc: 0.6216\n",
            "Epoch 79/1000\n",
            "84/84 [==============================] - 0s 166us/sample - loss: 8.0809e-04 - acc: 1.0000 - val_loss: 10.9981 - val_acc: 0.6216\n",
            "Epoch 80/1000\n",
            "84/84 [==============================] - 0s 183us/sample - loss: 0.0294 - acc: 0.9762 - val_loss: 8.4938 - val_acc: 0.6216\n",
            "Epoch 81/1000\n",
            "84/84 [==============================] - 0s 166us/sample - loss: 1.1393 - acc: 0.9643 - val_loss: 3.7203 - val_acc: 0.6486\n",
            "Epoch 82/1000\n",
            "84/84 [==============================] - 0s 150us/sample - loss: 0.0056 - acc: 1.0000 - val_loss: 3.9309 - val_acc: 0.6757\n",
            "Epoch 83/1000\n",
            "84/84 [==============================] - 0s 195us/sample - loss: 0.0055 - acc: 1.0000 - val_loss: 4.3290 - val_acc: 0.6486\n",
            "Epoch 84/1000\n",
            "84/84 [==============================] - 0s 252us/sample - loss: 0.6923 - acc: 0.9643 - val_loss: 1.8153 - val_acc: 0.7297\n",
            "Epoch 85/1000\n",
            "84/84 [==============================] - 0s 182us/sample - loss: 8.8295e-04 - acc: 1.0000 - val_loss: 1.7735 - val_acc: 0.7297\n",
            "Epoch 86/1000\n",
            "84/84 [==============================] - 0s 159us/sample - loss: 0.0678 - acc: 0.9881 - val_loss: 3.5390 - val_acc: 0.6757\n",
            "Epoch 87/1000\n",
            "84/84 [==============================] - 0s 214us/sample - loss: 0.5929 - acc: 0.9881 - val_loss: 13.2515 - val_acc: 0.5946\n",
            "Epoch 88/1000\n",
            "84/84 [==============================] - 0s 160us/sample - loss: 1.8713 - acc: 0.9881 - val_loss: 2.5897 - val_acc: 0.7027\n",
            "Epoch 89/1000\n",
            "84/84 [==============================] - 0s 163us/sample - loss: 0.0027 - acc: 1.0000 - val_loss: 2.8993 - val_acc: 0.7297\n",
            "Epoch 90/1000\n",
            "84/84 [==============================] - 0s 161us/sample - loss: 0.0038 - acc: 1.0000 - val_loss: 3.0689 - val_acc: 0.7297\n",
            "Epoch 91/1000\n",
            "84/84 [==============================] - 0s 203us/sample - loss: 0.0011 - acc: 1.0000 - val_loss: 2.6276 - val_acc: 0.7027\n",
            "Epoch 92/1000\n",
            "84/84 [==============================] - 0s 169us/sample - loss: 3.8982e-04 - acc: 1.0000 - val_loss: 2.6031 - val_acc: 0.7027\n",
            "Epoch 93/1000\n",
            "84/84 [==============================] - 0s 208us/sample - loss: 9.2027e-04 - acc: 1.0000 - val_loss: 2.5971 - val_acc: 0.7027\n",
            "Epoch 94/1000\n",
            "84/84 [==============================] - 0s 170us/sample - loss: 0.0034 - acc: 1.0000 - val_loss: 3.3946 - val_acc: 0.7027\n",
            "Epoch 95/1000\n",
            "84/84 [==============================] - 0s 162us/sample - loss: 2.6463e-04 - acc: 1.0000 - val_loss: 3.4466 - val_acc: 0.7027\n",
            "Epoch 96/1000\n",
            "84/84 [==============================] - 0s 237us/sample - loss: 6.2425e-04 - acc: 1.0000 - val_loss: 3.4640 - val_acc: 0.7027\n",
            "Epoch 97/1000\n",
            "84/84 [==============================] - 0s 214us/sample - loss: 4.5416e-04 - acc: 1.0000 - val_loss: 3.3949 - val_acc: 0.7027\n",
            "Epoch 98/1000\n",
            "84/84 [==============================] - 0s 153us/sample - loss: 1.2783e-04 - acc: 1.0000 - val_loss: 3.4003 - val_acc: 0.7027\n",
            "Epoch 99/1000\n",
            "84/84 [==============================] - 0s 177us/sample - loss: 0.0030 - acc: 1.0000 - val_loss: 6.9684 - val_acc: 0.6486\n",
            "Epoch 100/1000\n",
            "84/84 [==============================] - 0s 194us/sample - loss: 3.5783e-04 - acc: 1.0000 - val_loss: 6.9237 - val_acc: 0.6486\n",
            "Epoch 101/1000\n",
            "84/84 [==============================] - 0s 164us/sample - loss: 2.5421e-04 - acc: 1.0000 - val_loss: 6.5922 - val_acc: 0.6757\n",
            "Epoch 102/1000\n",
            "84/84 [==============================] - 0s 185us/sample - loss: 0.3206 - acc: 0.9762 - val_loss: 6.2487 - val_acc: 0.6757\n",
            "Epoch 103/1000\n",
            "84/84 [==============================] - 0s 210us/sample - loss: 0.0138 - acc: 0.9881 - val_loss: 7.7580 - val_acc: 0.6757\n",
            "Epoch 104/1000\n",
            "84/84 [==============================] - 0s 145us/sample - loss: 0.0399 - acc: 0.9881 - val_loss: 2.8015 - val_acc: 0.7297\n",
            "Epoch 105/1000\n",
            "84/84 [==============================] - 0s 174us/sample - loss: 0.0070 - acc: 1.0000 - val_loss: 4.3225 - val_acc: 0.7297\n",
            "Epoch 106/1000\n",
            "84/84 [==============================] - 0s 171us/sample - loss: 1.4984 - acc: 0.9643 - val_loss: 10.2446 - val_acc: 0.6216\n",
            "Epoch 107/1000\n",
            "84/84 [==============================] - 0s 172us/sample - loss: 0.0013 - acc: 1.0000 - val_loss: 11.2556 - val_acc: 0.6216\n",
            "Epoch 108/1000\n",
            "84/84 [==============================] - 0s 262us/sample - loss: 1.4054e-04 - acc: 1.0000 - val_loss: 11.2146 - val_acc: 0.6216\n",
            "Epoch 109/1000\n",
            "84/84 [==============================] - 0s 162us/sample - loss: 1.1851e-04 - acc: 1.0000 - val_loss: 11.1524 - val_acc: 0.6216\n",
            "Epoch 110/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 1.7019 - acc: 0.9881 - val_loss: 1.6514 - val_acc: 0.7568\n",
            "Epoch 111/1000\n",
            "84/84 [==============================] - 0s 172us/sample - loss: 5.8118e-05 - acc: 1.0000 - val_loss: 1.6559 - val_acc: 0.7568\n",
            "Epoch 112/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 1.8768e-04 - acc: 1.0000 - val_loss: 1.6723 - val_acc: 0.7297\n",
            "Epoch 113/1000\n",
            "84/84 [==============================] - 0s 179us/sample - loss: 8.2766e-04 - acc: 1.0000 - val_loss: 1.6939 - val_acc: 0.7297\n",
            "Epoch 114/1000\n",
            "84/84 [==============================] - 0s 174us/sample - loss: 0.0646 - acc: 0.9881 - val_loss: 1.7444 - val_acc: 0.7568\n",
            "Epoch 115/1000\n",
            "84/84 [==============================] - 0s 202us/sample - loss: 0.0266 - acc: 0.9881 - val_loss: 1.6579 - val_acc: 0.7568\n",
            "Epoch 116/1000\n",
            "84/84 [==============================] - 0s 217us/sample - loss: 0.3199 - acc: 0.9881 - val_loss: 5.1437 - val_acc: 0.7027\n",
            "Epoch 117/1000\n",
            "84/84 [==============================] - 0s 169us/sample - loss: 3.3067e-04 - acc: 1.0000 - val_loss: 5.0718 - val_acc: 0.7027\n",
            "Epoch 118/1000\n",
            "84/84 [==============================] - 0s 200us/sample - loss: 0.0541 - acc: 0.9881 - val_loss: 2.3915 - val_acc: 0.7027\n",
            "Epoch 119/1000\n",
            "84/84 [==============================] - 0s 192us/sample - loss: 3.7839e-05 - acc: 1.0000 - val_loss: 2.3939 - val_acc: 0.7297\n",
            "Epoch 120/1000\n",
            "84/84 [==============================] - 0s 229us/sample - loss: 0.1371 - acc: 0.9881 - val_loss: 8.5794 - val_acc: 0.7027\n",
            "Epoch 121/1000\n",
            "84/84 [==============================] - 0s 174us/sample - loss: 9.0691e-05 - acc: 1.0000 - val_loss: 8.5486 - val_acc: 0.6757\n",
            "Epoch 122/1000\n",
            "84/84 [==============================] - 0s 172us/sample - loss: 4.3522 - acc: 0.9643 - val_loss: 13.6429 - val_acc: 0.6216\n",
            "Epoch 123/1000\n",
            "84/84 [==============================] - 0s 197us/sample - loss: 6.3377e-04 - acc: 1.0000 - val_loss: 13.5128 - val_acc: 0.6486\n",
            "Epoch 124/1000\n",
            "84/84 [==============================] - 0s 161us/sample - loss: 0.2066 - acc: 0.9881 - val_loss: 15.0174 - val_acc: 0.6486\n",
            "Epoch 125/1000\n",
            "84/84 [==============================] - 0s 202us/sample - loss: 0.9947 - acc: 0.9881 - val_loss: 5.3988 - val_acc: 0.7027\n",
            "Epoch 126/1000\n",
            "84/84 [==============================] - 0s 173us/sample - loss: 0.0522 - acc: 0.9881 - val_loss: 6.3825 - val_acc: 0.7027\n",
            "Epoch 127/1000\n",
            "84/84 [==============================] - 0s 188us/sample - loss: 0.9310 - acc: 0.9881 - val_loss: 1.4425 - val_acc: 0.8108\n",
            "Epoch 128/1000\n",
            "84/84 [==============================] - 0s 169us/sample - loss: 0.0448 - acc: 0.9881 - val_loss: 2.7925 - val_acc: 0.7027\n",
            "Epoch 129/1000\n",
            "84/84 [==============================] - 0s 172us/sample - loss: 1.7871 - acc: 0.9881 - val_loss: 8.1767 - val_acc: 0.6486\n",
            "Epoch 130/1000\n",
            "84/84 [==============================] - 0s 155us/sample - loss: 4.9563e-04 - acc: 1.0000 - val_loss: 8.1652 - val_acc: 0.6486\n",
            "Epoch 131/1000\n",
            "84/84 [==============================] - 0s 188us/sample - loss: 4.7514e-05 - acc: 1.0000 - val_loss: 7.9909 - val_acc: 0.6486\n",
            "Epoch 132/1000\n",
            "84/84 [==============================] - 0s 154us/sample - loss: 1.7305e-05 - acc: 1.0000 - val_loss: 7.9655 - val_acc: 0.6486\n",
            "Epoch 133/1000\n",
            "84/84 [==============================] - 0s 162us/sample - loss: 5.4364 - acc: 0.9881 - val_loss: 2.0270 - val_acc: 0.7568\n",
            "Epoch 134/1000\n",
            "84/84 [==============================] - 0s 177us/sample - loss: 8.4938e-05 - acc: 1.0000 - val_loss: 2.0073 - val_acc: 0.7568\n",
            "Epoch 135/1000\n",
            "84/84 [==============================] - 0s 161us/sample - loss: 9.9804e-04 - acc: 1.0000 - val_loss: 2.4288 - val_acc: 0.6757\n",
            "Epoch 136/1000\n",
            "84/84 [==============================] - 0s 183us/sample - loss: 2.6169e-05 - acc: 1.0000 - val_loss: 2.4165 - val_acc: 0.6757\n",
            "Epoch 137/1000\n",
            "84/84 [==============================] - 0s 184us/sample - loss: 4.8955e-04 - acc: 1.0000 - val_loss: 2.5364 - val_acc: 0.6757\n",
            "Epoch 138/1000\n",
            "84/84 [==============================] - 0s 188us/sample - loss: 0.0051 - acc: 1.0000 - val_loss: 3.9925 - val_acc: 0.7027\n",
            "Epoch 139/1000\n",
            "84/84 [==============================] - 0s 157us/sample - loss: 0.3517 - acc: 0.9881 - val_loss: 15.7194 - val_acc: 0.6486\n",
            "Epoch 140/1000\n",
            "84/84 [==============================] - 0s 166us/sample - loss: 0.2350 - acc: 0.9881 - val_loss: 8.4747 - val_acc: 0.6486\n",
            "Epoch 141/1000\n",
            "84/84 [==============================] - 0s 261us/sample - loss: 5.7693e-05 - acc: 1.0000 - val_loss: 8.4420 - val_acc: 0.6757\n",
            "Epoch 142/1000\n",
            "84/84 [==============================] - 0s 195us/sample - loss: 0.1584 - acc: 0.9881 - val_loss: 6.1138 - val_acc: 0.6757\n",
            "Epoch 143/1000\n",
            "84/84 [==============================] - 0s 239us/sample - loss: 9.6167e-06 - acc: 1.0000 - val_loss: 6.1000 - val_acc: 0.6757\n",
            "Epoch 144/1000\n",
            "84/84 [==============================] - 0s 229us/sample - loss: 3.6247e-05 - acc: 1.0000 - val_loss: 6.1085 - val_acc: 0.6757\n",
            "Epoch 145/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 4.1078e-05 - acc: 1.0000 - val_loss: 6.0787 - val_acc: 0.6757\n",
            "Epoch 146/1000\n",
            "84/84 [==============================] - 0s 189us/sample - loss: 0.0578 - acc: 0.9881 - val_loss: 2.3616 - val_acc: 0.7568\n",
            "Epoch 147/1000\n",
            "84/84 [==============================] - 0s 186us/sample - loss: 0.0212 - acc: 0.9881 - val_loss: 8.1593 - val_acc: 0.6757\n",
            "Epoch 148/1000\n",
            "84/84 [==============================] - 0s 243us/sample - loss: 1.4895e-05 - acc: 1.0000 - val_loss: 8.1470 - val_acc: 0.6757\n",
            "Epoch 149/1000\n",
            "84/84 [==============================] - 0s 189us/sample - loss: 2.7687e-05 - acc: 1.0000 - val_loss: 8.1237 - val_acc: 0.6757\n",
            "Epoch 150/1000\n",
            "84/84 [==============================] - 0s 207us/sample - loss: 0.0844 - acc: 0.9881 - val_loss: 5.4196 - val_acc: 0.7297\n",
            "Epoch 151/1000\n",
            "84/84 [==============================] - 0s 167us/sample - loss: 9.6353e-06 - acc: 1.0000 - val_loss: 5.4256 - val_acc: 0.7297\n",
            "Epoch 152/1000\n",
            "84/84 [==============================] - 0s 212us/sample - loss: 4.7687e-04 - acc: 1.0000 - val_loss: 5.5280 - val_acc: 0.7297\n",
            "Epoch 153/1000\n",
            "84/84 [==============================] - 0s 215us/sample - loss: 5.9080e-05 - acc: 1.0000 - val_loss: 5.4820 - val_acc: 0.7297\n",
            "Epoch 154/1000\n",
            "84/84 [==============================] - 0s 213us/sample - loss: 1.0168e-04 - acc: 1.0000 - val_loss: 5.4922 - val_acc: 0.7568\n",
            "Epoch 155/1000\n",
            "84/84 [==============================] - 0s 200us/sample - loss: 1.0896e-05 - acc: 1.0000 - val_loss: 5.4841 - val_acc: 0.7568\n",
            "Epoch 156/1000\n",
            "84/84 [==============================] - 0s 242us/sample - loss: 2.5630e-05 - acc: 1.0000 - val_loss: 5.5527 - val_acc: 0.7297\n",
            "Epoch 157/1000\n",
            "84/84 [==============================] - 0s 212us/sample - loss: 4.1132e-04 - acc: 1.0000 - val_loss: 5.8131 - val_acc: 0.7297\n",
            "Epoch 158/1000\n",
            "84/84 [==============================] - 0s 175us/sample - loss: 8.1225e-06 - acc: 1.0000 - val_loss: 5.7963 - val_acc: 0.7297\n",
            "Epoch 159/1000\n",
            "84/84 [==============================] - 0s 229us/sample - loss: 2.0545e-05 - acc: 1.0000 - val_loss: 5.7946 - val_acc: 0.7297\n",
            "Epoch 160/1000\n",
            "84/84 [==============================] - 0s 261us/sample - loss: 7.9631e-04 - acc: 1.0000 - val_loss: 9.1855 - val_acc: 0.6486\n",
            "Epoch 161/1000\n",
            "84/84 [==============================] - 0s 231us/sample - loss: 8.0758e-06 - acc: 1.0000 - val_loss: 9.1954 - val_acc: 0.6486\n",
            "Epoch 162/1000\n",
            "84/84 [==============================] - 0s 178us/sample - loss: 4.2745e-04 - acc: 1.0000 - val_loss: 9.4510 - val_acc: 0.6486\n",
            "Epoch 163/1000\n",
            "84/84 [==============================] - 0s 194us/sample - loss: 3.6902e-05 - acc: 1.0000 - val_loss: 9.3938 - val_acc: 0.6757\n",
            "Epoch 164/1000\n",
            "84/84 [==============================] - 0s 194us/sample - loss: 3.4657e-05 - acc: 1.0000 - val_loss: 9.2596 - val_acc: 0.6757\n",
            "Epoch 165/1000\n",
            "84/84 [==============================] - 0s 170us/sample - loss: 6.4083e-06 - acc: 1.0000 - val_loss: 9.2589 - val_acc: 0.6757\n",
            "Epoch 166/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 0.2064 - acc: 0.9881 - val_loss: 2.4804 - val_acc: 0.7568\n",
            "Epoch 167/1000\n",
            "84/84 [==============================] - 0s 183us/sample - loss: 9.9026e-06 - acc: 1.0000 - val_loss: 2.4844 - val_acc: 0.7568\n",
            "Epoch 168/1000\n",
            "84/84 [==============================] - 0s 169us/sample - loss: 7.5567e-06 - acc: 1.0000 - val_loss: 2.4831 - val_acc: 0.7568\n",
            "Epoch 169/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 1.1074e-05 - acc: 1.0000 - val_loss: 2.4824 - val_acc: 0.7568\n",
            "Epoch 170/1000\n",
            "84/84 [==============================] - 0s 186us/sample - loss: 0.0727 - acc: 0.9881 - val_loss: 13.1964 - val_acc: 0.6216\n",
            "Epoch 171/1000\n",
            "84/84 [==============================] - 0s 208us/sample - loss: 0.0156 - acc: 0.9881 - val_loss: 9.5159 - val_acc: 0.6757\n",
            "Epoch 172/1000\n",
            "84/84 [==============================] - 0s 182us/sample - loss: 8.6257e-06 - acc: 1.0000 - val_loss: 9.5494 - val_acc: 0.6757\n",
            "Epoch 173/1000\n",
            "84/84 [==============================] - 0s 194us/sample - loss: 7.8801e-06 - acc: 1.0000 - val_loss: 9.5640 - val_acc: 0.6757\n",
            "Epoch 174/1000\n",
            "84/84 [==============================] - 0s 184us/sample - loss: 8.4263e-06 - acc: 1.0000 - val_loss: 9.5656 - val_acc: 0.6757\n",
            "Epoch 175/1000\n",
            "84/84 [==============================] - 0s 167us/sample - loss: 0.1198 - acc: 0.9881 - val_loss: 13.5400 - val_acc: 0.6757\n",
            "Epoch 176/1000\n",
            "84/84 [==============================] - 0s 160us/sample - loss: 0.0028 - acc: 1.0000 - val_loss: 4.4418 - val_acc: 0.7027\n",
            "Epoch 177/1000\n",
            "84/84 [==============================] - 0s 183us/sample - loss: 5.5880e-06 - acc: 1.0000 - val_loss: 4.4467 - val_acc: 0.7027\n",
            "Epoch 178/1000\n",
            "84/84 [==============================] - 0s 217us/sample - loss: 6.7087e-06 - acc: 1.0000 - val_loss: 4.4414 - val_acc: 0.7027\n",
            "Epoch 179/1000\n",
            "84/84 [==============================] - 0s 185us/sample - loss: 5.8749e-06 - acc: 1.0000 - val_loss: 4.4428 - val_acc: 0.7027\n",
            "Epoch 180/1000\n",
            "84/84 [==============================] - 0s 158us/sample - loss: 0.1933 - acc: 0.9881 - val_loss: 15.1954 - val_acc: 0.6757\n",
            "Epoch 181/1000\n",
            "84/84 [==============================] - 0s 174us/sample - loss: 5.0675e-06 - acc: 1.0000 - val_loss: 15.1837 - val_acc: 0.6757\n",
            "Epoch 182/1000\n",
            "84/84 [==============================] - 0s 203us/sample - loss: 4.3661e-05 - acc: 1.0000 - val_loss: 15.0965 - val_acc: 0.6757\n",
            "Epoch 183/1000\n",
            "84/84 [==============================] - 0s 171us/sample - loss: 0.5207 - acc: 0.9881 - val_loss: 4.2394 - val_acc: 0.7027\n",
            "Epoch 184/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 4.6503e-06 - acc: 1.0000 - val_loss: 4.2379 - val_acc: 0.7027\n",
            "Epoch 185/1000\n",
            "84/84 [==============================] - 0s 177us/sample - loss: 8.9574e-05 - acc: 1.0000 - val_loss: 4.6282 - val_acc: 0.7027\n",
            "Epoch 186/1000\n",
            "84/84 [==============================] - 0s 155us/sample - loss: 0.3072 - acc: 0.9881 - val_loss: 15.1954 - val_acc: 0.6486\n",
            "Epoch 187/1000\n",
            "84/84 [==============================] - 0s 163us/sample - loss: 2.9985e-06 - acc: 1.0000 - val_loss: 15.1810 - val_acc: 0.6486\n",
            "Epoch 188/1000\n",
            "84/84 [==============================] - 0s 178us/sample - loss: 0.0236 - acc: 0.9881 - val_loss: 10.8786 - val_acc: 0.6757\n",
            "Epoch 189/1000\n",
            "84/84 [==============================] - 0s 156us/sample - loss: 0.1515 - acc: 0.9881 - val_loss: 18.1159 - val_acc: 0.6486\n",
            "Epoch 190/1000\n",
            "84/84 [==============================] - 0s 172us/sample - loss: 0.1717 - acc: 0.9881 - val_loss: 7.0635 - val_acc: 0.6757\n",
            "Epoch 191/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 1.4149e-06 - acc: 1.0000 - val_loss: 7.0696 - val_acc: 0.6757\n",
            "Epoch 192/1000\n",
            "84/84 [==============================] - 0s 252us/sample - loss: 1.3024e-05 - acc: 1.0000 - val_loss: 7.0850 - val_acc: 0.6757\n",
            "Epoch 193/1000\n",
            "84/84 [==============================] - 0s 183us/sample - loss: 3.7222e-06 - acc: 1.0000 - val_loss: 7.0903 - val_acc: 0.6757\n",
            "Epoch 194/1000\n",
            "84/84 [==============================] - 0s 193us/sample - loss: 2.9582e-05 - acc: 1.0000 - val_loss: 7.1260 - val_acc: 0.6757\n",
            "Epoch 195/1000\n",
            "84/84 [==============================] - 0s 216us/sample - loss: 1.5838e-06 - acc: 1.0000 - val_loss: 7.1301 - val_acc: 0.6757\n",
            "Epoch 196/1000\n",
            "84/84 [==============================] - 0s 173us/sample - loss: 8.0813e-06 - acc: 1.0000 - val_loss: 7.1278 - val_acc: 0.6757\n",
            "Epoch 197/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 7.4439e-06 - acc: 1.0000 - val_loss: 7.1766 - val_acc: 0.6757\n",
            "Epoch 198/1000\n",
            "84/84 [==============================] - 0s 173us/sample - loss: 2.5544e-06 - acc: 1.0000 - val_loss: 7.1842 - val_acc: 0.6757\n",
            "Epoch 199/1000\n",
            "84/84 [==============================] - 0s 181us/sample - loss: 2.7843e-06 - acc: 1.0000 - val_loss: 7.1902 - val_acc: 0.6757\n",
            "Epoch 200/1000\n",
            "84/84 [==============================] - 0s 195us/sample - loss: 2.2053e-06 - acc: 1.0000 - val_loss: 7.1880 - val_acc: 0.6757\n",
            "Epoch 201/1000\n",
            "84/84 [==============================] - 0s 209us/sample - loss: 1.0033e-06 - acc: 1.0000 - val_loss: 7.1983 - val_acc: 0.6757\n",
            "Epoch 202/1000\n",
            "84/84 [==============================] - 0s 280us/sample - loss: 2.1414e-06 - acc: 1.0000 - val_loss: 7.2132 - val_acc: 0.6757\n",
            "Epoch 203/1000\n",
            "84/84 [==============================] - 0s 212us/sample - loss: 0.0310 - acc: 0.9881 - val_loss: 12.2019 - val_acc: 0.6757\n",
            "Epoch 204/1000\n",
            "84/84 [==============================] - 0s 242us/sample - loss: 0.3326 - acc: 0.9881 - val_loss: 8.6637 - val_acc: 0.7027\n",
            "Epoch 205/1000\n",
            "84/84 [==============================] - 0s 260us/sample - loss: 2.4778e-06 - acc: 1.0000 - val_loss: 8.7357 - val_acc: 0.7027\n",
            "Epoch 206/1000\n",
            "84/84 [==============================] - 0s 240us/sample - loss: 3.8046e-06 - acc: 1.0000 - val_loss: 8.7162 - val_acc: 0.7027\n",
            "Epoch 207/1000\n",
            "84/84 [==============================] - 0s 249us/sample - loss: 1.2318e-06 - acc: 1.0000 - val_loss: 8.7150 - val_acc: 0.7027\n",
            "Epoch 208/1000\n",
            "84/84 [==============================] - 0s 232us/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 9.0883 - val_acc: 0.6757\n",
            "Epoch 209/1000\n",
            "84/84 [==============================] - 0s 245us/sample - loss: 4.6083e-04 - acc: 1.0000 - val_loss: 7.1763 - val_acc: 0.6757\n",
            "Epoch 210/1000\n",
            "84/84 [==============================] - 0s 224us/sample - loss: 3.6909e-06 - acc: 1.0000 - val_loss: 7.1890 - val_acc: 0.6757\n",
            "Epoch 211/1000\n",
            "84/84 [==============================] - 0s 222us/sample - loss: 6.0456e-07 - acc: 1.0000 - val_loss: 7.1851 - val_acc: 0.6757\n",
            "Epoch 212/1000\n",
            "84/84 [==============================] - 0s 227us/sample - loss: 1.7739e-06 - acc: 1.0000 - val_loss: 7.1790 - val_acc: 0.6757\n",
            "Epoch 213/1000\n",
            "84/84 [==============================] - 0s 279us/sample - loss: 2.7630e-06 - acc: 1.0000 - val_loss: 7.1571 - val_acc: 0.6757\n",
            "Epoch 214/1000\n",
            "84/84 [==============================] - 0s 234us/sample - loss: 8.5310e-06 - acc: 1.0000 - val_loss: 7.1696 - val_acc: 0.6757\n",
            "Epoch 215/1000\n",
            "84/84 [==============================] - 0s 247us/sample - loss: 9.7778e-07 - acc: 1.0000 - val_loss: 7.1701 - val_acc: 0.6486\n",
            "Epoch 216/1000\n",
            "84/84 [==============================] - 0s 235us/sample - loss: 7.7060e-07 - acc: 1.0000 - val_loss: 7.1645 - val_acc: 0.6486\n",
            "Epoch 217/1000\n",
            "84/84 [==============================] - 0s 231us/sample - loss: 1.5388e-05 - acc: 1.0000 - val_loss: 7.1936 - val_acc: 0.6486\n",
            "Epoch 218/1000\n",
            "84/84 [==============================] - 0s 235us/sample - loss: 0.0172 - acc: 0.9881 - val_loss: 1.8960 - val_acc: 0.8108\n",
            "Epoch 219/1000\n",
            "84/84 [==============================] - 0s 232us/sample - loss: 0.3007 - acc: 0.9881 - val_loss: 11.4427 - val_acc: 0.6757\n",
            "Epoch 220/1000\n",
            "84/84 [==============================] - 0s 218us/sample - loss: 1.8789e-06 - acc: 1.0000 - val_loss: 11.3885 - val_acc: 0.7027\n",
            "Epoch 221/1000\n",
            "84/84 [==============================] - 0s 192us/sample - loss: 1.6263e-06 - acc: 1.0000 - val_loss: 11.3811 - val_acc: 0.7027\n",
            "Epoch 222/1000\n",
            "84/84 [==============================] - 0s 153us/sample - loss: 1.2639e-04 - acc: 1.0000 - val_loss: 10.3539 - val_acc: 0.7027\n",
            "Epoch 223/1000\n",
            "84/84 [==============================] - 0s 278us/sample - loss: 0.2057 - acc: 0.9881 - val_loss: 5.7219 - val_acc: 0.7027\n",
            "Epoch 224/1000\n",
            "84/84 [==============================] - 0s 264us/sample - loss: 0.0876 - acc: 0.9881 - val_loss: 15.2620 - val_acc: 0.7027\n",
            "Epoch 225/1000\n",
            "84/84 [==============================] - 0s 202us/sample - loss: 0.0170 - acc: 0.9881 - val_loss: 16.0453 - val_acc: 0.6757\n",
            "Epoch 226/1000\n",
            "84/84 [==============================] - 0s 280us/sample - loss: 9.5934e-07 - acc: 1.0000 - val_loss: 16.0351 - val_acc: 0.6757\n",
            "Epoch 227/1000\n",
            "84/84 [==============================] - 0s 210us/sample - loss: 4.3082e-06 - acc: 1.0000 - val_loss: 15.9799 - val_acc: 0.6757\n",
            "Epoch 228/1000\n",
            "84/84 [==============================] - 0s 206us/sample - loss: 0.5356 - acc: 0.9881 - val_loss: 5.0270 - val_acc: 0.7027\n",
            "Epoch 229/1000\n",
            "84/84 [==============================] - 0s 217us/sample - loss: 0.2079 - acc: 0.9643 - val_loss: 7.8805 - val_acc: 0.6757\n",
            "Epoch 230/1000\n",
            "84/84 [==============================] - 0s 180us/sample - loss: 1.9739e-06 - acc: 1.0000 - val_loss: 7.8791 - val_acc: 0.6757\n",
            "Epoch 231/1000\n",
            "84/84 [==============================] - 0s 208us/sample - loss: 1.4365 - acc: 0.9881 - val_loss: 14.2081 - val_acc: 0.6757\n",
            "Epoch 232/1000\n",
            "84/84 [==============================] - 0s 225us/sample - loss: 7.1241e-07 - acc: 1.0000 - val_loss: 14.2156 - val_acc: 0.6757\n",
            "Epoch 233/1000\n",
            "84/84 [==============================] - 0s 209us/sample - loss: 8.3729e-07 - acc: 1.0000 - val_loss: 14.2039 - val_acc: 0.6757\n",
            "Epoch 234/1000\n",
            "84/84 [==============================] - 0s 242us/sample - loss: 3.3659e-04 - acc: 1.0000 - val_loss: 12.6486 - val_acc: 0.6486\n",
            "Epoch 235/1000\n",
            "84/84 [==============================] - 0s 200us/sample - loss: 1.7036e-04 - acc: 1.0000 - val_loss: 12.4342 - val_acc: 0.6486\n",
            "Epoch 236/1000\n",
            "84/84 [==============================] - 0s 167us/sample - loss: 8.5006e-07 - acc: 1.0000 - val_loss: 12.3644 - val_acc: 0.6486\n",
            "Epoch 237/1000\n",
            "84/84 [==============================] - 0s 206us/sample - loss: 4.8961e-07 - acc: 1.0000 - val_loss: 12.3646 - val_acc: 0.6486\n",
            "Epoch 238/1000\n",
            "84/84 [==============================] - 0s 218us/sample - loss: 4.5413e-07 - acc: 1.0000 - val_loss: 12.3644 - val_acc: 0.6486\n",
            "Epoch 239/1000\n",
            "84/84 [==============================] - 0s 216us/sample - loss: 1.2261e-06 - acc: 1.0000 - val_loss: 12.3533 - val_acc: 0.6486\n",
            "Epoch 240/1000\n",
            "84/84 [==============================] - 0s 171us/sample - loss: 4.5554e-07 - acc: 1.0000 - val_loss: 12.3509 - val_acc: 0.6486\n",
            "Epoch 241/1000\n",
            "84/84 [==============================] - 0s 181us/sample - loss: 6.4865e-05 - acc: 1.0000 - val_loss: 12.9866 - val_acc: 0.6757\n",
            "Epoch 242/1000\n",
            "84/84 [==============================] - 0s 163us/sample - loss: 1.4081 - acc: 0.9881 - val_loss: 4.9823 - val_acc: 0.7027\n",
            "Epoch 243/1000\n",
            "84/84 [==============================] - 0s 219us/sample - loss: 9.0967e-07 - acc: 1.0000 - val_loss: 4.9879 - val_acc: 0.7027\n",
            "Epoch 244/1000\n",
            "84/84 [==============================] - 0s 217us/sample - loss: 3.9452e-07 - acc: 1.0000 - val_loss: 4.9885 - val_acc: 0.7027\n",
            "Epoch 245/1000\n",
            "84/84 [==============================] - 0s 172us/sample - loss: 6.4977e-06 - acc: 1.0000 - val_loss: 5.0027 - val_acc: 0.7027\n",
            "Epoch 246/1000\n",
            "84/84 [==============================] - 0s 204us/sample - loss: 2.3842e-07 - acc: 1.0000 - val_loss: 5.0008 - val_acc: 0.7027\n",
            "Epoch 247/1000\n",
            "84/84 [==============================] - 0s 209us/sample - loss: 3.8885e-07 - acc: 1.0000 - val_loss: 5.0041 - val_acc: 0.7027\n",
            "Epoch 248/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 2.5261e-07 - acc: 1.0000 - val_loss: 5.0083 - val_acc: 0.7027\n",
            "Epoch 249/1000\n",
            "84/84 [==============================] - 0s 174us/sample - loss: 6.2661e-06 - acc: 1.0000 - val_loss: 5.0261 - val_acc: 0.6757\n",
            "Epoch 250/1000\n",
            "84/84 [==============================] - 0s 195us/sample - loss: 0.4835 - acc: 0.9881 - val_loss: 16.0383 - val_acc: 0.6757\n",
            "Epoch 251/1000\n",
            "84/84 [==============================] - 0s 199us/sample - loss: 2.6437e-06 - acc: 1.0000 - val_loss: 15.9111 - val_acc: 0.6757\n",
            "Epoch 252/1000\n",
            "84/84 [==============================] - 0s 190us/sample - loss: 4.0029e-06 - acc: 1.0000 - val_loss: 15.6957 - val_acc: 0.6486\n",
            "Epoch 253/1000\n",
            "84/84 [==============================] - 0s 188us/sample - loss: 2.1599e-06 - acc: 1.0000 - val_loss: 15.6827 - val_acc: 0.6486\n",
            "Epoch 254/1000\n",
            "84/84 [==============================] - 0s 179us/sample - loss: 0.1887 - acc: 0.9881 - val_loss: 9.1474 - val_acc: 0.6757\n",
            "Epoch 255/1000\n",
            "84/84 [==============================] - 0s 182us/sample - loss: 0.0045 - acc: 1.0000 - val_loss: 11.0746 - val_acc: 0.6757\n",
            "Epoch 256/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 3.1505e-07 - acc: 1.0000 - val_loss: 11.0693 - val_acc: 0.6757\n",
            "Epoch 257/1000\n",
            "84/84 [==============================] - 0s 235us/sample - loss: 1.0699e-05 - acc: 1.0000 - val_loss: 10.9964 - val_acc: 0.7027\n",
            "Epoch 258/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 3.5621e-07 - acc: 1.0000 - val_loss: 10.9862 - val_acc: 0.7027\n",
            "Epoch 259/1000\n",
            "84/84 [==============================] - 0s 229us/sample - loss: 9.0116e-07 - acc: 1.0000 - val_loss: 10.9904 - val_acc: 0.7027\n",
            "Epoch 260/1000\n",
            "84/84 [==============================] - 0s 250us/sample - loss: 9.4089e-07 - acc: 1.0000 - val_loss: 10.9650 - val_acc: 0.7027\n",
            "Epoch 261/1000\n",
            "84/84 [==============================] - 0s 189us/sample - loss: 5.9888e-07 - acc: 1.0000 - val_loss: 10.9256 - val_acc: 0.7027\n",
            "Epoch 262/1000\n",
            "84/84 [==============================] - 0s 165us/sample - loss: 6.7267e-07 - acc: 1.0000 - val_loss: 10.9300 - val_acc: 0.7027\n",
            "Epoch 263/1000\n",
            "84/84 [==============================] - 0s 189us/sample - loss: 6.8545e-07 - acc: 1.0000 - val_loss: 10.9297 - val_acc: 0.7027\n",
            "Epoch 264/1000\n",
            "84/84 [==============================] - 0s 170us/sample - loss: 5.5205e-07 - acc: 1.0000 - val_loss: 10.9141 - val_acc: 0.7027\n",
            "Epoch 265/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 9.1535e-07 - acc: 1.0000 - val_loss: 10.8372 - val_acc: 0.7027\n",
            "Epoch 266/1000\n",
            "84/84 [==============================] - 0s 173us/sample - loss: 6.4003e-07 - acc: 1.0000 - val_loss: 10.8252 - val_acc: 0.7027\n",
            "Epoch 267/1000\n",
            "84/84 [==============================] - 0s 182us/sample - loss: 5.0096e-07 - acc: 1.0000 - val_loss: 10.8276 - val_acc: 0.7027\n",
            "Epoch 268/1000\n",
            "84/84 [==============================] - 0s 212us/sample - loss: 1.2843e-06 - acc: 1.0000 - val_loss: 10.8139 - val_acc: 0.7027\n",
            "Epoch 269/1000\n",
            "84/84 [==============================] - 0s 235us/sample - loss: 0.1269 - acc: 0.9881 - val_loss: 2.5955 - val_acc: 0.7297\n",
            "Epoch 270/1000\n",
            "84/84 [==============================] - 0s 157us/sample - loss: 0.8019 - acc: 0.9881 - val_loss: 8.0356 - val_acc: 0.6757\n",
            "Epoch 271/1000\n",
            "84/84 [==============================] - 0s 179us/sample - loss: 8.5817e-06 - acc: 1.0000 - val_loss: 8.5792 - val_acc: 0.6757\n",
            "Epoch 272/1000\n",
            "84/84 [==============================] - 0s 215us/sample - loss: 6.6955e-05 - acc: 1.0000 - val_loss: 7.6039 - val_acc: 0.6757\n",
            "Epoch 273/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 4.9386e-07 - acc: 1.0000 - val_loss: 7.5879 - val_acc: 0.6757\n",
            "Epoch 274/1000\n",
            "84/84 [==============================] - 0s 168us/sample - loss: 1.1240e-04 - acc: 1.0000 - val_loss: 7.9717 - val_acc: 0.6757\n",
            "Epoch 275/1000\n",
            "84/84 [==============================] - 0s 170us/sample - loss: 1.0644e-07 - acc: 1.0000 - val_loss: 7.9702 - val_acc: 0.6757\n",
            "Epoch 276/1000\n",
            "84/84 [==============================] - 0s 197us/sample - loss: 3.1505e-07 - acc: 1.0000 - val_loss: 7.9775 - val_acc: 0.6757\n",
            "Epoch 277/1000\n",
            "84/84 [==============================] - 0s 153us/sample - loss: 5.4522e-05 - acc: 1.0000 - val_loss: 8.4974 - val_acc: 0.7027\n",
            "Epoch 278/1000\n",
            "84/84 [==============================] - 0s 173us/sample - loss: 9.2243e-07 - acc: 1.0000 - val_loss: 8.4933 - val_acc: 0.7027\n",
            "Epoch 279/1000\n",
            "84/84 [==============================] - 0s 181us/sample - loss: 3.7891e-07 - acc: 1.0000 - val_loss: 8.4882 - val_acc: 0.7027\n",
            "Epoch 280/1000\n",
            "84/84 [==============================] - 0s 190us/sample - loss: 1.5099e-06 - acc: 1.0000 - val_loss: 8.4400 - val_acc: 0.6757\n",
            "Epoch 281/1000\n",
            "84/84 [==============================] - 0s 279us/sample - loss: 3.5053e-07 - acc: 1.0000 - val_loss: 8.4312 - val_acc: 0.6757\n",
            "Epoch 282/1000\n",
            "84/84 [==============================] - 0s 187us/sample - loss: 0.0039 - acc: 1.0000 - val_loss: 6.1479 - val_acc: 0.6486\n",
            "Epoch 283/1000\n",
            "84/84 [==============================] - 0s 238us/sample - loss: 4.6548e-07 - acc: 1.0000 - val_loss: 6.1403 - val_acc: 0.6486\n",
            "Epoch 284/1000\n",
            "84/84 [==============================] - 0s 192us/sample - loss: 1.7710e-06 - acc: 1.0000 - val_loss: 6.2964 - val_acc: 0.6757\n",
            "Epoch 285/1000\n",
            "84/84 [==============================] - 0s 180us/sample - loss: 3.1221e-07 - acc: 1.0000 - val_loss: 6.2988 - val_acc: 0.6757\n",
            "Epoch 286/1000\n",
            "84/84 [==============================] - 0s 190us/sample - loss: 2.9518e-07 - acc: 1.0000 - val_loss: 6.2069 - val_acc: 0.6757\n",
            "Epoch 287/1000\n",
            "84/84 [==============================] - 0s 241us/sample - loss: 0.5805 - acc: 0.9762 - val_loss: 4.9003 - val_acc: 0.7027\n",
            "Epoch 288/1000\n",
            "84/84 [==============================] - 0s 205us/sample - loss: 0.0946 - acc: 0.9881 - val_loss: 8.2203 - val_acc: 0.6757\n",
            "Epoch 289/1000\n",
            "84/84 [==============================] - 0s 185us/sample - loss: 8.8128e-07 - acc: 1.0000 - val_loss: 8.1939 - val_acc: 0.6757\n",
            "Epoch 290/1000\n",
            "84/84 [==============================] - 0s 233us/sample - loss: 4.3568e-07 - acc: 1.0000 - val_loss: 8.1709 - val_acc: 0.6757\n",
            "Epoch 291/1000\n",
            "84/84 [==============================] - 0s 204us/sample - loss: 1.3723e-06 - acc: 1.0000 - val_loss: 8.1508 - val_acc: 0.6757\n",
            "Epoch 292/1000\n",
            "84/84 [==============================] - 0s 184us/sample - loss: 6.5281e-07 - acc: 1.0000 - val_loss: 8.2919 - val_acc: 0.6757\n",
            "Epoch 293/1000\n",
            "84/84 [==============================] - 0s 222us/sample - loss: 2.4977e-07 - acc: 1.0000 - val_loss: 8.2876 - val_acc: 0.6757\n",
            "Epoch 294/1000\n",
            "84/84 [==============================] - 0s 220us/sample - loss: 4.2574e-07 - acc: 1.0000 - val_loss: 8.2787 - val_acc: 0.6757\n",
            "Epoch 295/1000\n",
            "84/84 [==============================] - 0s 197us/sample - loss: 2.9944e-07 - acc: 1.0000 - val_loss: 8.2642 - val_acc: 0.6757\n",
            "Epoch 296/1000\n",
            "84/84 [==============================] - 0s 232us/sample - loss: 3.7891e-07 - acc: 1.0000 - val_loss: 8.2396 - val_acc: 0.6757\n",
            "Epoch 297/1000\n",
            "84/84 [==============================] - 0s 188us/sample - loss: 1.1765e-06 - acc: 1.0000 - val_loss: 8.2022 - val_acc: 0.6757\n",
            "Epoch 298/1000\n",
            "84/84 [==============================] - 0s 199us/sample - loss: 1.6216e-05 - acc: 1.0000 - val_loss: 9.1837 - val_acc: 0.6757\n",
            "Epoch 299/1000\n",
            "84/84 [==============================] - 0s 200us/sample - loss: 3.4202e-07 - acc: 1.0000 - val_loss: 9.1816 - val_acc: 0.6757\n",
            "Epoch 300/1000\n",
            "84/84 [==============================] - 0s 244us/sample - loss: 1.9853e-06 - acc: 1.0000 - val_loss: 9.1844 - val_acc: 0.6757\n",
            "Epoch 301/1000\n",
            "84/84 [==============================] - 0s 201us/sample - loss: 4.0020e-07 - acc: 1.0000 - val_loss: 9.1741 - val_acc: 0.6757\n",
            "Epoch 302/1000\n",
            "84/84 [==============================] - 0s 200us/sample - loss: 7.2376e-07 - acc: 1.0000 - val_loss: 9.1226 - val_acc: 0.7027\n",
            "Epoch 303/1000\n",
            "84/84 [==============================] - 0s 191us/sample - loss: 5.4211e-07 - acc: 1.0000 - val_loss: 9.1288 - val_acc: 0.7027\n",
            "Epoch 304/1000\n",
            "84/84 [==============================] - 0s 214us/sample - loss: 1.5014e-06 - acc: 1.0000 - val_loss: 9.1425 - val_acc: 0.7027\n",
            "Epoch 305/1000\n",
            "84/84 [==============================] - 0s 263us/sample - loss: 1.5043e-07 - acc: 1.0000 - val_loss: 9.1224 - val_acc: 0.7027\n",
            "Epoch 306/1000\n",
            "84/84 [==============================] - 0s 258us/sample - loss: 1.9301e-07 - acc: 1.0000 - val_loss: 9.1227 - val_acc: 0.7027\n",
            "Epoch 307/1000\n",
            "84/84 [==============================] - 0s 234us/sample - loss: 7.0389e-07 - acc: 1.0000 - val_loss: 9.1733 - val_acc: 0.7027\n",
            "Epoch 308/1000\n",
            "84/84 [==============================] - 0s 207us/sample - loss: 1.3198e-07 - acc: 1.0000 - val_loss: 9.1574 - val_acc: 0.7027\n",
            "Epoch 309/1000\n",
            "84/84 [==============================] - 0s 237us/sample - loss: 1.9159e-07 - acc: 1.0000 - val_loss: 9.1457 - val_acc: 0.7027\n",
            "Epoch 310/1000\n",
            "84/84 [==============================] - 0s 311us/sample - loss: 4.1297e-07 - acc: 1.0000 - val_loss: 9.1097 - val_acc: 0.7027\n",
            "Epoch 311/1000\n",
            "84/84 [==============================] - 0s 140us/sample - loss: 1.3482e-07 - acc: 1.0000 - val_loss: 9.1055 - val_acc: 0.7027\n",
            "Epoch 312/1000\n",
            "84/84 [==============================] - 0s 302us/sample - loss: 1.3340e-07 - acc: 1.0000 - val_loss: 9.0941 - val_acc: 0.7027\n",
            "Epoch 313/1000\n",
            "84/84 [==============================] - 0s 219us/sample - loss: 1.7739e-07 - acc: 1.0000 - val_loss: 9.0706 - val_acc: 0.7027\n",
            "Epoch 314/1000\n",
            "84/84 [==============================] - 0s 228us/sample - loss: 3.7466e-07 - acc: 1.0000 - val_loss: 9.0359 - val_acc: 0.7027\n",
            "Epoch 315/1000\n",
            "84/84 [==============================] - 0s 198us/sample - loss: 1.0502e-07 - acc: 1.0000 - val_loss: 9.0302 - val_acc: 0.7027\n",
            "Epoch 316/1000\n",
            "84/84 [==============================] - 0s 212us/sample - loss: 4.0588e-07 - acc: 1.0000 - val_loss: 9.1216 - val_acc: 0.6757\n",
            "Epoch 317/1000\n",
            "84/84 [==============================] - 0s 268us/sample - loss: 3.2357e-07 - acc: 1.0000 - val_loss: 8.7400 - val_acc: 0.6757\n",
            "Epoch 318/1000\n",
            "84/84 [==============================] - 0s 270us/sample - loss: 6.1024e-08 - acc: 1.0000 - val_loss: 8.7322 - val_acc: 0.6757\n",
            "Epoch 319/1000\n",
            "84/84 [==============================] - 0s 235us/sample - loss: 4.8251e-08 - acc: 1.0000 - val_loss: 8.7319 - val_acc: 0.6757\n",
            "Epoch 320/1000\n",
            "84/84 [==============================] - 0s 238us/sample - loss: 1.3482e-07 - acc: 1.0000 - val_loss: 8.7463 - val_acc: 0.6757\n",
            "Epoch 321/1000\n",
            "84/84 [==============================] - 0s 250us/sample - loss: 1.2347e-07 - acc: 1.0000 - val_loss: 8.7233 - val_acc: 0.6757\n",
            "Epoch 322/1000\n",
            "84/84 [==============================] - 0s 176us/sample - loss: 1.4759e-07 - acc: 1.0000 - val_loss: 8.7019 - val_acc: 0.6757\n",
            "Epoch 323/1000\n",
            "84/84 [==============================] - 0s 242us/sample - loss: 1.4192e-07 - acc: 1.0000 - val_loss: 8.6859 - val_acc: 0.6757\n",
            "Epoch 324/1000\n",
            "84/84 [==============================] - 0s 178us/sample - loss: 6.5281e-08 - acc: 1.0000 - val_loss: 8.6670 - val_acc: 0.6757\n",
            "Epoch 325/1000\n",
            "84/84 [==============================] - 0s 181us/sample - loss: 3.5479e-08 - acc: 1.0000 - val_loss: 8.6450 - val_acc: 0.6757\n",
            "Epoch 326/1000\n",
            "84/84 [==============================] - 0s 199us/sample - loss: 5.2509e-08 - acc: 1.0000 - val_loss: 8.6161 - val_acc: 0.6486\n",
            "Epoch 327/1000\n",
            "84/84 [==============================] - 0s 1ms/sample - loss: 7.0958e-08 - acc: 1.0000 - val_loss: 8.6022 - val_acc: 0.6486\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNCsFwdBkD64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36efc69-e6bf-4ea6-cb7f-68e4343f6cbc"
      },
      "source": [
        "# Model prediction accuracy on the test data (it's same as validation data)\n",
        "print(\"Accuracy on Test Data:\", model.evaluate(x_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on Test Data: [1.4424756346522152, 0.8108108]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTaBTUgwkLTH"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/RealLife_DeceptionDetection/MultiModelConcatinatedModel_HM\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4pi7_PGkOw4"
      },
      "source": [
        "# model.save(\"MyModelfile\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YsDvWf8kSoo"
      },
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/RealLife_DeceptionDetection/MultimodelConvatinated_weights/weightsC_HM\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}