{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUMb2nmsksE"
      },
      "source": [
        "# Transcript Classification (Deception Detection)\n",
        "\n",
        "Based on the transcript, wheter a person lying or saying truth is calassified using the convolutional neural networks.\n",
        "\n",
        "However, before building the model, transcripts should be processed such that model can find usefull information.\n",
        "\n",
        "Following are the steps:\n",
        "\n",
        "   - Read Transcripts and Store it in variable\n",
        "   - Load Pretrained word2vector data (GloVe -840B)\n",
        "   - Convert Transcripts to matrix using word2vect\n",
        "   - Split datasets in 70% - 30% ratio\n",
        "   - Build model and train\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fzsXmMqsksH"
      },
      "source": [
        "# Import Libraries\n",
        "\n",
        "For building CNN machine learning model __Tensorflow__ library is used. For performing specific tasks such as tokenizing, vectorizing, word2vector etc. modules of __Tensorflow__ library is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl7S2kVKsksI"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "\n",
        "import numpy as np  # for numerical operations and matrix like datatype\n",
        "import os           # for accessing directory related functions\n",
        "\n",
        "# Importing functions from tensorflow library\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization # for making text vector\n",
        "\n",
        "from tensorflow.keras.layers import Embedding  # For embedding the sentences in to numerical matrix\n",
        "from tensorflow.keras import layers,models     # For various layers of Machine Learning model (i.e. Conv)\n",
        "from tensorflow import keras                   # For building machine learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_mNza0FsksJ"
      },
      "source": [
        "# Read Transcripts\n",
        "\n",
        "For each hearing, there is individual text file that contrains the transcript. And are in seperate folder.\n",
        "So first all the transcripts are loaded in one single variable.\n",
        "\n",
        "Folder structure is assumed as follows (based on visual inspection)\n",
        "\n",
        "- Truthfull/\n",
        "    - trial_truth_001.txt\n",
        "    - ...\n",
        "- Deceptive/\n",
        "    - trial_lie_001.txt\n",
        "    - ...\n",
        "\n",
        "Here using __os__ library for getting the list of the filenames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y5g6Z0BsksJ"
      },
      "outputs": [],
      "source": [
        "# Getting the list of File Names\n",
        "\n",
        "Truthful = \"Truthful\"   # These are the folder names where transcripts were placed\n",
        "Deceptive = \"Deceptive\" \n",
        "\n",
        "truth_ids = [fname[:-4] for fname in os.listdir(Truthful)]  # Tacking the empty list and will only store the file name without file extention.\n",
        "lie_ids = [fname[:-4] for fname in os.listdir(Deceptive)]   # As Id will be same for coresponding video file and other informations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAWlOSxesksK"
      },
      "source": [
        "## Read all files and store in a list\n",
        "\n",
        "Once all the file name are fetched, transcripts are stored in list and then that list is converted to numpy array for futher convinence. Another List is created that will store the class of the transcript (i.e. true = 1 or lie = 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhoMX1SUsksL"
      },
      "outputs": [],
      "source": [
        "# Now Read those files and store in list\n",
        "\n",
        "Truth_sentences = []\n",
        "Lie_sentences = []\n",
        "\n",
        "# First reading the truthfull transcripts\n",
        "for fname in truth_ids:\n",
        "    path = os.path.join(Truthful,fname+\".txt\")\n",
        "    with open(path) as f:\n",
        "        Truth_sentences.append(f.read())\n",
        "        \n",
        "# Reading lie transcripts\n",
        "for fname in lie_ids:\n",
        "    path = os.path.join(Deceptive,fname+\".txt\")\n",
        "    with open(path) as f:\n",
        "        Lie_sentences.append(f.read())\n",
        "        \n",
        "# Labels for classification\n",
        "\n",
        "Truth_labels = [1 for _ in range(len(Truth_sentences))]\n",
        "Lie_labels = [0 for _ in range(len(Lie_sentences))]\n",
        "\n",
        "# Converting in numpy array\n",
        "Truth_sentences = np.array(Truth_sentences).reshape(len(Truth_sentences),1)  # Column vector\n",
        "Lie_sentences = np.array(Lie_sentences).reshape(len(Lie_sentences),1)\n",
        "\n",
        "Truth_labels = np.array(Truth_labels).reshape(len(Truth_labels),1)  # Column vector\n",
        "Lie_labels = np.array(Lie_labels).reshape(len(Lie_labels),1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oK_P18OQsksL"
      },
      "source": [
        "## Merge both type of list in one\n",
        "\n",
        "Once transcripts and labels were sotred in aproriate list (or numpy array), it is joined in one numpy array consisting two column: First one cosists of transcripts and second one is it's label(i.e. true=1 or lie = 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztFdIZKAsksM"
      },
      "outputs": [],
      "source": [
        "# Now Store in numpy array as : First column is transcript and Second one its class (i.e. truth or lie)\n",
        "\n",
        "T_data = np.concatenate([Truth_sentences,Truth_labels],axis=1)  # Connect 2 vector vertically\n",
        "L_data = np.concatenate([Lie_sentences,Lie_labels],axis = 1)  # Connect 2 vector vertically\n",
        "\n",
        "AllData = np.concatenate([T_data,L_data],axis=0)  # Connecte 2 matrix Horizontally (i.e. appending another matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrGs7Hl0sksM",
        "outputId": "a211f328-d7ca-497b-b2ef-a6f5a86a90c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([['That word has a definition and you are not using it. You are not using the definition that I apply in this ... in this continuum. So if you want me to talk about what stalking means, I will talk about it.',\n",
              "        '0'],\n",
              "       ['The lunging and the gun going off were sort of contemporanios, I don’t remember how close they were or if it happened at exactly the same moment or one right after the other, it all happened very fast, and it all seemed to happen all at once, and I would say as far as distance, maybe as far as Mr. Babbikey (sp?) is, (court reporter?) but I couldn’t say for sure with absolute certainty.',\n",
              "        '0']], dtype='<U1071')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "AllData[-2:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsEuviQ6sksN"
      },
      "source": [
        "# Word Vectorization\n",
        "\n",
        "For machine learning, text data needs to be converted into numerical values with appropriate methods. One of which is Word2Vector method where each word is represented by a __n__-dimentional vector. So a sentence(transcript) with __m__ words is represented by either __n__ x __m__ or __m__ x __n__ sized matrix. \n",
        "\n",
        "To represent a word in vector an unsurevised learning methods is used or a pretrained data is used. So, here pretrained data from __GloVe__ :Global Vector for word representation is used for getting vector for word. from the __GloVe__ dataset __300__-dimetional(i.e. __n=300__) representation is used.\n",
        "\n",
        "Since each transcript will be having of different legth, So here window of __100__ words is used (i.e. __m=100__). transcript with number of words less than __100__ is padded with zero and larger is trimmed.\n",
        "\n",
        "So, a transcript is represented by __n__ x __m__ = __300__ x __100__ matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ur3gLc7sksN"
      },
      "source": [
        "## Loading GloVe dataset\n",
        "\n",
        "First, load all the data from pre-trained dataset in to a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4PUdzGJFsksN",
        "outputId": "8b261b97-f376-4214-9810-2b5b03c1c1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words in GloVe dataset: 2196016\n"
          ]
        }
      ],
      "source": [
        "# Loading pretrained GloVe dataset\n",
        "glove_folder = \"glove.840B\"\n",
        "glove_file = \"glove.840B.300d.txt\"\n",
        "glove_path = os.path.join(glove_folder,glove_file)\n",
        "\n",
        "embodided_word_map = {}\n",
        "\n",
        "with open(glove_path) as emFile:\n",
        "    for line in emFile:\n",
        "        word,vect = line.split(\" \",maxsplit=1)\n",
        "        vect = np.fromstring(vect, \"f\", sep=\" \")\n",
        "        embodided_word_map[word] = vect\n",
        "\n",
        "print(\"Total words in GloVe dataset:\",len(embodided_word_map))\n",
        "# embodided_word_map['hi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "O0I5a-KDsksO",
        "outputId": "a69499b9-f6e6-457b-be51-52f75fe96cef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-1.9463e-02, -1.8862e-01, -3.3833e-01, -1.7087e-02,  2.4807e-01,\n",
              "       -2.0557e-01,  1.9839e-01,  9.0633e-03, -1.8412e-01,  2.1553e+00,\n",
              "       -3.5654e-01, -1.8052e-01,  4.8173e-02, -2.7695e-01,  5.6454e-02,\n",
              "        1.6258e-01, -2.7082e-01,  1.0765e+00, -4.1729e-01, -3.3334e-01,\n",
              "        5.8293e-03, -2.1324e-01,  3.2689e-01, -2.0474e-01, -1.8690e-01,\n",
              "        2.3764e-01, -3.5091e-02, -1.0563e-01,  2.1216e-01, -1.8023e-01,\n",
              "       -3.4032e-01, -4.8700e-02, -1.1078e-01,  6.8588e-02,  2.5711e-01,\n",
              "       -1.4287e-01, -4.4981e-02,  1.0357e-01, -3.3532e-01, -1.9495e-01,\n",
              "       -3.3474e-01, -1.5415e-01,  1.8489e-01, -1.4937e-01,  2.8578e-01,\n",
              "       -2.1299e-01, -4.9552e-01, -1.8745e-01, -7.4939e-02,  5.0816e-02,\n",
              "       -3.5211e-02,  1.4748e-01,  1.0345e-01, -4.2498e-01,  2.1406e-01,\n",
              "        1.4149e-01, -2.4607e-01, -4.4894e-02,  2.4726e-01,  1.2828e-01,\n",
              "        1.6653e-01, -4.6914e-01, -1.5911e-01,  3.3017e-01,  4.4409e-02,\n",
              "        1.5897e-01, -2.3019e-01,  2.3289e-01,  5.7056e-01,  9.2753e-04,\n",
              "        2.8964e-01, -5.1595e-02,  6.5910e-01,  2.8264e-01,  9.4405e-02,\n",
              "        1.7922e-01, -5.5770e-02, -3.9942e-01, -2.2483e-01, -5.1352e-02,\n",
              "        8.4707e-02,  4.4268e-01,  2.0552e-01,  7.1328e-02,  2.0297e-01,\n",
              "       -3.3700e-01, -1.1153e-01,  5.1338e-02,  2.9456e-01,  9.1528e-02,\n",
              "        4.8475e-03,  4.6691e-01, -2.0435e-01,  4.1117e-02,  4.4357e-01,\n",
              "        1.1083e-01,  2.7820e-02, -1.9758e-01,  2.1313e-01, -1.8897e-01,\n",
              "        1.1105e-01,  4.2969e-01,  4.6931e-02, -1.2998e-01,  2.8806e-01,\n",
              "       -6.3154e-01,  4.8623e-02, -2.1371e-01,  1.2003e-01, -3.5273e-02,\n",
              "       -2.6883e-01, -5.1553e-01,  3.6962e-01, -1.0965e-01,  1.8084e-01,\n",
              "        1.4466e-01, -1.1034e-01, -3.3349e-01,  6.2657e-02, -2.5227e-01,\n",
              "       -1.6011e-02, -6.2980e-02, -4.7280e-01,  1.9845e-01,  1.6713e-01,\n",
              "       -8.6287e-02,  4.8028e-02, -5.4392e-01,  4.7845e-02, -2.2357e-01,\n",
              "       -3.0508e-01, -1.1440e-01, -3.0234e-01,  2.3010e-01,  1.2619e-01,\n",
              "        1.8492e-01, -3.4214e-01, -2.5361e-01, -2.1193e-01,  8.3952e-02,\n",
              "       -2.1643e+00,  3.8554e-01,  7.5564e-02, -2.1658e-01,  1.6353e-02,\n",
              "       -2.9314e-01, -7.9956e-02,  2.5078e-01, -9.2895e-03,  7.7943e-02,\n",
              "        2.0687e-01, -1.2354e-01,  1.3575e-01, -1.3058e-01, -4.5661e-02,\n",
              "        3.0147e-01, -1.7943e-01, -2.8461e-01,  5.5522e-02, -4.2376e-01,\n",
              "       -1.3030e-01,  1.0166e-02, -3.9183e-01, -1.7898e-02,  1.9834e-02,\n",
              "       -2.3901e-01,  1.4123e-01, -1.2442e-01,  4.4602e-01, -7.9938e-02,\n",
              "       -1.9608e-01, -8.1335e-02,  7.8286e-03, -2.3900e-01, -3.3480e-02,\n",
              "        4.5167e-02,  5.1881e-02,  4.7891e-01, -2.0277e-01,  3.6585e-01,\n",
              "       -1.9003e-01,  1.2260e-01, -2.3080e-01, -3.2798e-01, -1.1299e-01,\n",
              "        2.5676e-01, -1.1365e-01, -1.1697e-01, -1.1216e-01,  3.0418e-01,\n",
              "        8.9566e-02,  1.3347e-01, -2.7849e-02, -2.8675e-01,  2.2906e-01,\n",
              "        1.8339e-01, -1.1955e-01, -4.8542e-02,  1.2207e-01,  4.2335e-01,\n",
              "       -3.2219e-01,  6.7723e-02,  1.2396e-02, -2.5142e-01, -1.1566e-01,\n",
              "        7.0925e-02,  2.4792e-02,  7.2444e-02, -2.4386e-01,  3.4036e-01,\n",
              "       -9.0406e-02, -3.3420e-01, -9.4826e-02, -6.2298e-01,  6.9618e-02,\n",
              "        4.1616e-01, -4.0495e-02, -6.7077e-03, -2.8532e-02, -2.5219e-01,\n",
              "        5.2706e-02, -1.5360e-01, -3.9033e-01,  4.1211e-01,  2.6733e-01,\n",
              "       -4.8209e-02, -4.1589e-01,  3.8545e-01,  1.7734e-01, -1.4923e-01,\n",
              "       -4.3178e-01,  1.7102e-01, -2.0183e-02,  3.2373e-01,  2.7056e-01,\n",
              "       -2.2020e-01, -2.3979e-02, -3.2167e-01,  2.1605e-01,  4.6224e-01,\n",
              "       -1.5610e-01,  3.1712e-02, -3.2321e-01,  2.5788e-02,  2.1202e-01,\n",
              "        7.6985e-02, -1.0305e-01,  3.6565e-02, -3.6225e-02,  5.2611e-01,\n",
              "        6.4999e-02, -4.4936e-01, -1.6205e-01, -4.0973e-03, -6.7655e-02,\n",
              "        7.2511e-02,  4.8332e-01,  2.8505e-01, -1.3755e-01,  7.5515e-02,\n",
              "       -5.8013e-04,  5.1055e-01,  3.3834e-01,  2.6971e-01,  4.5874e-01,\n",
              "       -4.0463e-02, -1.7633e-02,  1.1928e-01,  4.5421e-01,  7.0101e-01,\n",
              "       -4.9884e-01, -6.0238e-03,  1.8800e-01, -2.7856e-01, -1.5845e-01,\n",
              "       -6.6627e-02,  3.3896e-01,  1.4628e-02, -2.7649e-02,  1.5310e-01,\n",
              "        1.1580e-01,  9.3094e-02, -3.0807e-01,  8.0278e-02, -6.4697e-02,\n",
              "       -3.1722e-01, -1.0506e-01, -9.5186e-03,  1.8897e-01, -3.0407e-01,\n",
              "       -4.3385e-01,  1.9794e-01,  1.4157e-01, -1.7506e-01, -1.6881e-01,\n",
              "       -1.1683e-02, -2.2626e-01, -5.0390e-02,  3.5910e-01,  4.5369e-01],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example:- \n",
        "embodided_word_map['look']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xc8RIOA5sksO"
      },
      "source": [
        "## Get list of unique words from all transcripts (Vocabulary)\n",
        "\n",
        "To get the vocabulary of our transcripts, Vectorization module from tensorflow is used. That will find all the unique words from all the transcripts as well as can perform simpel vectorization with the help of __one-hot encoding__ which is representing a word by it's index in Vocabulary. Here we can all specify the length of output vector (__m__)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Mzk0YoxsksO",
        "outputId": "5ace095c-f9f3-48a4-deae-a5464d7e2d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of Vocabulary 1521\n"
          ]
        }
      ],
      "source": [
        "# creating a function that can create vector of one-hot encoding and learns the vocabulary.\n",
        "# Here, from each sentences only 100 words are considered, so extra is excluded and padded with 0 when less\n",
        "\n",
        "make_vector = TextVectorization(output_sequence_length = 100) # m = 100\n",
        "make_vector.adapt(AllData[:,0])  # Adapting to out dataset\n",
        "\n",
        "vocab = make_vector.get_vocabulary()\n",
        "vocab_idx = dict(zip(vocab,range(len(vocab))))\n",
        "print(\"Length of Vocabulary\",len(vocab_idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d-ZYQG99sksO",
        "outputId": "84248261-c959-43cd-ff0e-d424a294b80b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'': 0,\n",
              " '[UNK]': 1,\n",
              " 'i': 2,\n",
              " 'and': 3,\n",
              " 'the': 4,\n",
              " 'to': 5,\n",
              " 'was': 6,\n",
              " 'that': 7,\n",
              " 'a': 8,\n",
              " 'he': 9,\n",
              " 'of': 10,\n",
              " 'it': 11,\n",
              " 'in': 12,\n",
              " 'my': 13,\n",
              " 'me': 14,\n",
              " 'um': 15,\n",
              " 'you': 16,\n",
              " 'so': 17,\n",
              " 'just': 18,\n",
              " 'had': 19,\n",
              " 'on': 20,\n",
              " 'we': 21,\n",
              " 'they': 22,\n",
              " 'is': 23,\n",
              " 'but': 24,\n",
              " 'know': 25,\n",
              " 'have': 26,\n",
              " 'at': 27,\n",
              " 'what': 28,\n",
              " 'like': 29,\n",
              " 'him': 30,\n",
              " 'not': 31,\n",
              " 'for': 32,\n",
              " 'as': 33,\n",
              " 'there': 34,\n",
              " 'uh': 35,\n",
              " 'were': 36,\n",
              " 'this': 37,\n",
              " 'time': 38,\n",
              " 'with': 39,\n",
              " 'then': 40,\n",
              " 'she': 41,\n",
              " 'when': 42,\n",
              " 'up': 43,\n",
              " 'did': 44,\n",
              " 'all': 45,\n",
              " 'about': 46,\n",
              " 'out': 47,\n",
              " 'or': 48,\n",
              " 'do': 49,\n",
              " 'her': 50,\n",
              " 'said': 51,\n",
              " 'didnt': 52,\n",
              " 'would': 53,\n",
              " 'get': 54,\n",
              " 'dont': 55,\n",
              " 'back': 56,\n",
              " 'if': 57,\n",
              " 'going': 58,\n",
              " 'remember': 59,\n",
              " 'no': 60,\n",
              " 'really': 61,\n",
              " 'are': 62,\n",
              " 'his': 63,\n",
              " 'because': 64,\n",
              " 'be': 65,\n",
              " 'two': 66,\n",
              " 'been': 67,\n",
              " '…': 68,\n",
              " 'well': 69,\n",
              " 'told': 70,\n",
              " 'started': 71,\n",
              " 'other': 72,\n",
              " 'its': 73,\n",
              " 'got': 74,\n",
              " 'go': 75,\n",
              " 'very': 76,\n",
              " 'through': 77,\n",
              " 'them': 78,\n",
              " 'right': 79,\n",
              " 'im': 80,\n",
              " 'house': 81,\n",
              " 'from': 82,\n",
              " 'an': 83,\n",
              " 'something': 84,\n",
              " 'one': 85,\n",
              " 'down': 86,\n",
              " 'years': 87,\n",
              " 'went': 88,\n",
              " 'think': 89,\n",
              " 'say': 90,\n",
              " 'never': 91,\n",
              " 'by': 92,\n",
              " 'yes': 93,\n",
              " 'where': 94,\n",
              " 'thats': 95,\n",
              " 'over': 96,\n",
              " 'how': 97,\n",
              " 'happened': 98,\n",
              " 'some': 99,\n",
              " 'could': 100,\n",
              " 'ammm': 101,\n",
              " 'way': 102,\n",
              " 'want': 103,\n",
              " 'things': 104,\n",
              " 'people': 105,\n",
              " 'more': 106,\n",
              " 'mean': 107,\n",
              " 'little': 108,\n",
              " 'kind': 109,\n",
              " 'even': 110,\n",
              " 'asked': 111,\n",
              " 'who': 112,\n",
              " 'whenever': 113,\n",
              " 'talk': 114,\n",
              " 'put': 115,\n",
              " 'into': 116,\n",
              " 'don’t': 117,\n",
              " 'day': 118,\n",
              " 'car': 119,\n",
              " 'being': 120,\n",
              " 'any': 121,\n",
              " 'after': 122,\n",
              " 'us': 123,\n",
              " 'point': 124,\n",
              " 'lot': 125,\n",
              " 'ground': 126,\n",
              " 'didn’t': 127,\n",
              " 'can': 128,\n",
              " 'around': 129,\n",
              " 'why': 130,\n",
              " 'trying': 131,\n",
              " 'thought': 132,\n",
              " 'tell': 133,\n",
              " 'sure': 134,\n",
              " 'sort': 135,\n",
              " 'see': 136,\n",
              " 'much': 137,\n",
              " 'felt': 138,\n",
              " 'come': 139,\n",
              " 'call': 140,\n",
              " 'work': 141,\n",
              " 'which': 142,\n",
              " 'wasnt': 143,\n",
              " 'until': 144,\n",
              " 'saw': 145,\n",
              " 'our': 146,\n",
              " 'off': 147,\n",
              " 'made': 148,\n",
              " 'long': 149,\n",
              " 'here': 150,\n",
              " 'head': 151,\n",
              " 'floor': 152,\n",
              " 'face': 153,\n",
              " 'again': 154,\n",
              " 'will': 155,\n",
              " 'wanted': 156,\n",
              " 'took': 157,\n",
              " 'those': 158,\n",
              " 'take': 159,\n",
              " 'someone': 160,\n",
              " 'once': 161,\n",
              " 'oh': 162,\n",
              " 'make': 163,\n",
              " 'life': 164,\n",
              " 'knew': 165,\n",
              " 'gun': 166,\n",
              " 'good': 167,\n",
              " 'gonna': 168,\n",
              " 'first': 169,\n",
              " 'family': 170,\n",
              " 'dance': 171,\n",
              " 'called': 172,\n",
              " 'believe': 173,\n",
              " 'anything': 174,\n",
              " 'ahhh': 175,\n",
              " 'ah': 176,\n",
              " 'word': 177,\n",
              " 'umm': 178,\n",
              " 'times': 179,\n",
              " 'three': 180,\n",
              " 'telling': 181,\n",
              " 'still': 182,\n",
              " 'side': 183,\n",
              " 'shot': 184,\n",
              " 'same': 185,\n",
              " 'room': 186,\n",
              " 'only': 187,\n",
              " 'officer': 188,\n",
              " 'nothing': 189,\n",
              " 'maybe': 190,\n",
              " 'man': 191,\n",
              " 'looking': 192,\n",
              " 'lived': 193,\n",
              " 'left': 194,\n",
              " 'jodi': 195,\n",
              " 'it’s': 196,\n",
              " 'investigation': 197,\n",
              " 'happen': 198,\n",
              " 'gas': 199,\n",
              " 'evidence': 200,\n",
              " 'door': 201,\n",
              " 'cause': 202,\n",
              " 'cant': 203,\n",
              " 'came': 204,\n",
              " 'both': 205,\n",
              " 'best': 206,\n",
              " 'asking': 207,\n",
              " 'always': 208,\n",
              " 'alone': 209,\n",
              " 'walked': 210,\n",
              " 'tried': 211,\n",
              " 'too': 212,\n",
              " 'together': 213,\n",
              " 'thing': 214,\n",
              " 'their': 215,\n",
              " 'than': 216,\n",
              " 'talked': 217,\n",
              " 'standing': 218,\n",
              " 'somebody': 219,\n",
              " 'sense': 220,\n",
              " 'school': 221,\n",
              " 'saying': 222,\n",
              " 'rusty': 223,\n",
              " 'run': 224,\n",
              " 'present': 225,\n",
              " 'police': 226,\n",
              " 'phone': 227,\n",
              " 'officers': 228,\n",
              " 'office': 229,\n",
              " 'now': 230,\n",
              " 'next': 231,\n",
              " 'neck': 232,\n",
              " 'mother': 233,\n",
              " 'look': 234,\n",
              " 'ive': 235,\n",
              " 'idea': 236,\n",
              " 'heard': 237,\n",
              " 'getting': 238,\n",
              " 'friends': 239,\n",
              " 'fell': 240,\n",
              " 'far': 241,\n",
              " 'everybody': 242,\n",
              " 'er': 243,\n",
              " 'doing': 244,\n",
              " 'body': 245,\n",
              " 'also': 246,\n",
              " 'actually': 247,\n",
              " 'yep': 248,\n",
              " 'yeah': 249,\n",
              " 'words': 250,\n",
              " 'woman': 251,\n",
              " 'while': 252,\n",
              " 'walk': 253,\n",
              " 'understand': 254,\n",
              " 'thinking': 255,\n",
              " 'theres': 256,\n",
              " 'that’s': 257,\n",
              " 'talking': 258,\n",
              " 'step': 259,\n",
              " 'sitting': 260,\n",
              " 'sir': 261,\n",
              " 'several': 262,\n",
              " 'seemed': 263,\n",
              " 'scared': 264,\n",
              " 'relationship': 265,\n",
              " 'project': 266,\n",
              " 'pretty': 267,\n",
              " 'person': 268,\n",
              " 'open': 269,\n",
              " 'okay': 270,\n",
              " 'noticed': 271,\n",
              " 'nobody': 272,\n",
              " 'new': 273,\n",
              " 'need': 274,\n",
              " 'myself': 275,\n",
              " 'mr': 276,\n",
              " 'months': 277,\n",
              " 'mathew': 278,\n",
              " 'last': 279,\n",
              " 'kill': 280,\n",
              " 'jamie': 281,\n",
              " 'isnt': 282,\n",
              " 'initially': 283,\n",
              " 'hood': 284,\n",
              " 'hemy': 285,\n",
              " 'help': 286,\n",
              " 'gone': 287,\n",
              " 'girls': 288,\n",
              " 'garage': 289,\n",
              " 'feel': 290,\n",
              " 'exactly': 291,\n",
              " 'ended': 292,\n",
              " 'else': 293,\n",
              " 'done': 294,\n",
              " 'couldnt': 295,\n",
              " 'conte': 296,\n",
              " 'completely': 297,\n",
              " 'clear': 298,\n",
              " 'chair': 299,\n",
              " 'case': 300,\n",
              " 'care': 301,\n",
              " 'candace': 302,\n",
              " 'brother': 303,\n",
              " 'away': 304,\n",
              " 'ask': 305,\n",
              " 'armor': 306,\n",
              " 'already': 307,\n",
              " 'accident': 308,\n",
              " 'absolutely': 309,\n",
              " 'able': 310,\n",
              " 'your': 311,\n",
              " 'young': 312,\n",
              " 'wonderful': 313,\n",
              " 'whats': 314,\n",
              " 'whatever': 315,\n",
              " 'wasn’t': 316,\n",
              " 'value': 317,\n",
              " 'try': 318,\n",
              " 'travis': 319,\n",
              " 'tim': 320,\n",
              " 'terrible': 321,\n",
              " 'ten': 322,\n",
              " 'taken': 323,\n",
              " 'system': 324,\n",
              " 'stuff': 325,\n",
              " 'street': 326,\n",
              " 'stopped': 327,\n",
              " 'staff': 328,\n",
              " 'spoken': 329,\n",
              " 'spent': 330,\n",
              " 'sometimes': 331,\n",
              " 'shut': 332,\n",
              " 'sentenced': 333,\n",
              " 'sentence': 334,\n",
              " 'seat': 335,\n",
              " 'screaming': 336,\n",
              " 'scene': 337,\n",
              " 'rifle': 338,\n",
              " 'remorse': 339,\n",
              " 'recall': 340,\n",
              " 'ran': 341,\n",
              " 'questions': 342,\n",
              " 'putting': 343,\n",
              " 'push': 344,\n",
              " 'process': 345,\n",
              " 'probably': 346,\n",
              " 'prision': 347,\n",
              " 'period': 348,\n",
              " 'percent': 349,\n",
              " 'own': 350,\n",
              " 'old': 351,\n",
              " 'none': 352,\n",
              " 'nine': 353,\n",
              " 'night': 354,\n",
              " 'nicole': 355,\n",
              " 'needed': 356,\n",
              " 'name': 357,\n",
              " 'moment': 358,\n",
              " 'mom': 359,\n",
              " 'might': 360,\n",
              " 'memory': 361,\n",
              " 'mc': 362,\n",
              " 'married': 363,\n",
              " 'many': 364,\n",
              " 'lying': 365,\n",
              " 'loved': 366,\n",
              " 'looked': 367,\n",
              " 'law': 368,\n",
              " 'laura': 369,\n",
              " 'kinda': 370,\n",
              " 'kids': 371,\n",
              " 'individual': 372,\n",
              " 'incident': 373,\n",
              " 'image': 374,\n",
              " 'id': 375,\n",
              " 'hung': 376,\n",
              " 'hard': 377,\n",
              " 'hadnt': 378,\n",
              " 'guy': 379,\n",
              " 'guess': 380,\n",
              " 'goes': 381,\n",
              " 'front': 382,\n",
              " 'finally': 383,\n",
              " 'fast': 384,\n",
              " 'explained': 385,\n",
              " 'explain': 386,\n",
              " 'donalds': 387,\n",
              " 'dna': 388,\n",
              " 'distance': 389,\n",
              " 'disappearance': 390,\n",
              " 'dead': 391,\n",
              " 'days': 392,\n",
              " 'crime': 393,\n",
              " 'couple': 394,\n",
              " 'convicted': 395,\n",
              " 'conversation': 396,\n",
              " 'close': 397,\n",
              " 'choking': 398,\n",
              " 'child': 399,\n",
              " 'changed': 400,\n",
              " 'cells': 401,\n",
              " 'business': 402,\n",
              " 'bit': 403,\n",
              " 'before': 404,\n",
              " 'bedroom': 405,\n",
              " 'bathroom': 406,\n",
              " 'basically': 407,\n",
              " 'bad': 408,\n",
              " 'area': 409,\n",
              " 'anyone': 410,\n",
              " 'ago': 411,\n",
              " '10th': 412,\n",
              " '100': 413,\n",
              " '“don’t': 414,\n",
              " 'youre': 415,\n",
              " 'younger': 416,\n",
              " 'year': 417,\n",
              " 'yall': 418,\n",
              " 'wrong': 419,\n",
              " 'wounds': 420,\n",
              " 'wouldnt': 421,\n",
              " 'without': 422,\n",
              " 'whole': 423,\n",
              " 'white': 424,\n",
              " 'whether': 425,\n",
              " 'werent': 426,\n",
              " 'warrant': 427,\n",
              " 'wall': 428,\n",
              " 'violence': 429,\n",
              " 'using': 430,\n",
              " 'used': 431,\n",
              " 'use': 432,\n",
              " 'under': 433,\n",
              " 'uncle': 434,\n",
              " 'ummm': 435,\n",
              " 'typically': 436,\n",
              " 'twitter': 437,\n",
              " 'turning': 438,\n",
              " 'truth': 439,\n",
              " 'tripped': 440,\n",
              " 'trial': 441,\n",
              " 'transfer': 442,\n",
              " 'transaction': 443,\n",
              " 'towards': 444,\n",
              " 'toward': 445,\n",
              " 'top': 446,\n",
              " 'threw': 447,\n",
              " 'texas': 448,\n",
              " 'testify': 449,\n",
              " 'talks': 450,\n",
              " 'taking': 451,\n",
              " 'supposed': 452,\n",
              " 'such': 453,\n",
              " 'struggled': 454,\n",
              " 'strong': 455,\n",
              " 'stick': 456,\n",
              " 'stayed': 457,\n",
              " 'stay': 458,\n",
              " 'state': 459,\n",
              " 'stalking': 460,\n",
              " 'spot': 461,\n",
              " 'sperm': 462,\n",
              " 'sorry': 463,\n",
              " 'son': 464,\n",
              " 'somewhere': 465,\n",
              " 'solid': 466,\n",
              " 'society': 467,\n",
              " 'smelled': 468,\n",
              " 'six': 469,\n",
              " 'sink': 470,\n",
              " 'show': 471,\n",
              " 'shooting': 472,\n",
              " 'shes': 473,\n",
              " 'sent': 474,\n",
              " 'seem': 475,\n",
              " 'second': 476,\n",
              " 'screamed': 477,\n",
              " 'scratched': 478,\n",
              " 'says': 479,\n",
              " 'safety': 480,\n",
              " 'rug': 481,\n",
              " 'rounds': 482,\n",
              " 'rolled': 483,\n",
              " 'road': 484,\n",
              " 'responsible': 485,\n",
              " 'responding': 486,\n",
              " 'responded': 487,\n",
              " 'respect': 488,\n",
              " 'realized': 489,\n",
              " 'real': 490,\n",
              " 'rape': 491,\n",
              " 'quote': 492,\n",
              " 'quickly': 493,\n",
              " 'pushed': 494,\n",
              " 'pulling': 495,\n",
              " 'pull': 496,\n",
              " 'prison': 497,\n",
              " 'position': 498,\n",
              " 'pops': 499,\n",
              " 'placing': 500,\n",
              " 'place': 501,\n",
              " 'pizza': 502,\n",
              " 'pictures': 503,\n",
              " 'picking': 504,\n",
              " 'pick': 505,\n",
              " 'perfect': 506,\n",
              " 'panicked': 507,\n",
              " 'orders': 508,\n",
              " 'order': 509,\n",
              " 'online': 510,\n",
              " 'older': 511,\n",
              " 'occurred': 512,\n",
              " 'objective': 513,\n",
              " 'nope': 514,\n",
              " 'most': 515,\n",
              " 'mind': 516,\n",
              " 'mexico': 517,\n",
              " 'mentioned': 518,\n",
              " 'members': 519,\n",
              " 'media': 520,\n",
              " 'matters': 521,\n",
              " 'making': 522,\n",
              " 'loveseat': 523,\n",
              " 'lost': 524,\n",
              " 'living': 525,\n",
              " 'live': 526,\n",
              " 'letters': 527,\n",
              " 'later': 528,\n",
              " 'lacis': 529,\n",
              " 'kitchen': 530,\n",
              " 'killed': 531,\n",
              " 'kept': 532,\n",
              " 'join': 533,\n",
              " 'job': 534,\n",
              " 'items': 535,\n",
              " 'involved': 536,\n",
              " 'intoxicated': 537,\n",
              " 'individuals': 538,\n",
              " 'inaudible': 539,\n",
              " 'immediately': 540,\n",
              " 'imma': 541,\n",
              " 'ian': 542,\n",
              " 'husband': 543,\n",
              " 'hundred': 544,\n",
              " 'hit': 545,\n",
              " 'hes': 546,\n",
              " 'hell': 547,\n",
              " 'has': 548,\n",
              " 'hands': 549,\n",
              " 'hand': 550,\n",
              " 'hallway': 551,\n",
              " 'hair': 552,\n",
              " 'guys': 553,\n",
              " 'great': 554,\n",
              " 'grant': 555,\n",
              " 'grade': 556,\n",
              " 'grabbed': 557,\n",
              " 'gmail': 558,\n",
              " 'glasses': 559,\n",
              " 'give': 560,\n",
              " 'garbage': 561,\n",
              " 'free': 562,\n",
              " 'forty': 563,\n",
              " 'forth': 564,\n",
              " 'forget': 565,\n",
              " 'following': 566,\n",
              " 'folk': 567,\n",
              " 'fog': 568,\n",
              " 'flirt': 569,\n",
              " 'five': 570,\n",
              " 'fine': 571,\n",
              " 'find': 572,\n",
              " 'figure': 573,\n",
              " 'feet': 574,\n",
              " 'father': 575,\n",
              " 'false': 576,\n",
              " 'fairly': 577,\n",
              " 'f': 578,\n",
              " 'exonerated': 579,\n",
              " 'every': 580,\n",
              " 'ever': 581,\n",
              " 'entire': 582,\n",
              " 'enough': 583,\n",
              " 'either': 584,\n",
              " 'eight': 585,\n",
              " 'edify': 586,\n",
              " 'each': 587,\n",
              " 'driving': 588,\n",
              " 'driveway': 589,\n",
              " 'drive': 590,\n",
              " 'drinking': 591,\n",
              " 'dreamed': 592,\n",
              " 'dorsey': 593,\n",
              " 'does': 594,\n",
              " 'different': 595,\n",
              " 'determine': 596,\n",
              " 'detectives': 597,\n",
              " 'deployed': 598,\n",
              " 'department': 599,\n",
              " 'definition': 600,\n",
              " 'defending': 601,\n",
              " 'debris': 602,\n",
              " 'death': 603,\n",
              " 'deal': 604,\n",
              " 'dancing': 605,\n",
              " 'dad': 606,\n",
              " 'criminal': 607,\n",
              " 'coward': 608,\n",
              " 'couldn’t': 609,\n",
              " 'correct': 610,\n",
              " 'cornrows': 611,\n",
              " 'corner': 612,\n",
              " 'computer': 613,\n",
              " 'committed': 614,\n",
              " 'coming': 615,\n",
              " 'club': 616,\n",
              " 'class': 617,\n",
              " 'change': 618,\n",
              " 'chairs': 619,\n",
              " 'certainly': 620,\n",
              " 'castle': 621,\n",
              " 'bumped': 622,\n",
              " 'breath': 623,\n",
              " 'born': 624,\n",
              " 'boat': 625,\n",
              " 'bloop': 626,\n",
              " 'blood': 627,\n",
              " 'blank': 628,\n",
              " 'black': 629,\n",
              " 'big': 630,\n",
              " 'between': 631,\n",
              " 'better': 632,\n",
              " 'believed': 633,\n",
              " 'began': 634,\n",
              " 'battery': 635,\n",
              " 'basketball': 636,\n",
              " 'based': 637,\n",
              " 'band': 638,\n",
              " 'badge': 639,\n",
              " 'backed': 640,\n",
              " 'baby': 641,\n",
              " 'aunt': 642,\n",
              " 'attempt': 643,\n",
              " 'assume': 644,\n",
              " 'approached': 645,\n",
              " 'anywhere': 646,\n",
              " 'anyway': 647,\n",
              " 'another': 648,\n",
              " 'am': 649,\n",
              " 'alexander': 650,\n",
              " 'additional': 651,\n",
              " '7th': 652,\n",
              " '4th': 653,\n",
              " '“no': 654,\n",
              " '‘hi': 655,\n",
              " 'youd': 656,\n",
              " 'yesterday': 657,\n",
              " 'yelling': 658,\n",
              " 'yelled': 659,\n",
              " 'wrote': 660,\n",
              " 'written': 661,\n",
              " 'worshiping': 662,\n",
              " 'wood': 663,\n",
              " 'wont': 664,\n",
              " 'woke': 665,\n",
              " 'withheld': 666,\n",
              " 'wish': 667,\n",
              " 'win': 668,\n",
              " 'william': 669,\n",
              " 'who’s': 670,\n",
              " 'wheres': 671,\n",
              " 'what’s': 672,\n",
              " 'werethey': 673,\n",
              " 'weresightransferring': 674,\n",
              " 'weren’t': 675,\n",
              " 'weimerskirtz': 676,\n",
              " 'washaving': 677,\n",
              " 'wanna': 678,\n",
              " 'wandered': 679,\n",
              " 'waited': 680,\n",
              " 'waistband': 681,\n",
              " 'w': 682,\n",
              " 'vulnerable': 683,\n",
              " 'vise': 684,\n",
              " 'virginia': 685,\n",
              " 'violent': 686,\n",
              " 'view': 687,\n",
              " 'victims': 688,\n",
              " 'vicinity': 689,\n",
              " 'versa': 690,\n",
              " 'velez': 691,\n",
              " 'vehicleandi': 692,\n",
              " 'vantage': 693,\n",
              " 'valley': 694,\n",
              " 'vacation': 695,\n",
              " 'usually': 696,\n",
              " 'ups': 697,\n",
              " 'upon': 698,\n",
              " 'undetermined': 699,\n",
              " 'understandable': 700,\n",
              " 'umwell': 701,\n",
              " 'umwe': 702,\n",
              " 'umand': 703,\n",
              " 'ukraine': 704,\n",
              " 'uhto': 705,\n",
              " 'uhm': 706,\n",
              " 'uhhh': 707,\n",
              " 'type': 708,\n",
              " 'tylenol': 709,\n",
              " 'twirled': 710,\n",
              " 'twently': 711,\n",
              " 'twelve': 712,\n",
              " 'tweeting': 713,\n",
              " 'turned': 714,\n",
              " 'turmoil': 715,\n",
              " 'tshirt': 716,\n",
              " 'trust': 717,\n",
              " 'trunk': 718,\n",
              " 'truly': 719,\n",
              " 'tree': 720,\n",
              " 'treats': 721,\n",
              " 'treated': 722,\n",
              " 'travis’': 723,\n",
              " 'traits': 724,\n",
              " 'training': 725,\n",
              " 'trained': 726,\n",
              " 'town': 727,\n",
              " 'totally': 728,\n",
              " 'total': 729,\n",
              " 'torso': 730,\n",
              " 'tore': 731,\n",
              " 'topping': 732,\n",
              " 'tobarriss': 733,\n",
              " 'tire': 734,\n",
              " 'tied': 735,\n",
              " 'throughout': 736,\n",
              " 'throat': 737,\n",
              " 'thousand': 738,\n",
              " 'thorough': 739,\n",
              " 'thirty': 740,\n",
              " 'they’ll': 741,\n",
              " 'theyd': 742,\n",
              " 'therein': 743,\n",
              " 'therefore': 744,\n",
              " 'themselves': 745,\n",
              " 'text’': 746,\n",
              " 'texting': 747,\n",
              " 'text': 748,\n",
              " 'testified': 749,\n",
              " 'tested': 750,\n",
              " 'test': 751,\n",
              " 'terms': 752,\n",
              " 'temple': 753,\n",
              " 'telephoned': 754,\n",
              " 'tee': 755,\n",
              " 'taught': 756,\n",
              " 'taser': 757,\n",
              " 'tank': 758,\n",
              " 'tanbark': 759,\n",
              " 'taller': 760,\n",
              " 'talkative': 761,\n",
              " 'takenand': 762,\n",
              " 'tactical': 763,\n",
              " 'table': 764,\n",
              " 'swallow': 765,\n",
              " 'supports': 766,\n",
              " 'sunglasses': 767,\n",
              " 'summary': 768,\n",
              " 'suicide': 769,\n",
              " 'suffer': 770,\n",
              " 'successful': 771,\n",
              " 'stutters': 772,\n",
              " 'struggling': 773,\n",
              " 'straightforward': 774,\n",
              " 'straddled': 775,\n",
              " 'story': 776,\n",
              " 'store': 777,\n",
              " 'stood': 778,\n",
              " 'statements': 779,\n",
              " 'statement': 780,\n",
              " 'stated': 781,\n",
              " 'stash': 782,\n",
              " 'start': 783,\n",
              " 'staring': 784,\n",
              " 'stapler': 785,\n",
              " 'stadium': 786,\n",
              " 'stability': 787,\n",
              " 'ssure': 788,\n",
              " 'spo': 789,\n",
              " 'split': 790,\n",
              " 'speaker': 791,\n",
              " 'space': 792,\n",
              " 'sp': 793,\n",
              " 'sounds': 794,\n",
              " 'sounded': 795,\n",
              " 'sound': 796,\n",
              " 'sophomore': 797,\n",
              " 'soon': 798,\n",
              " 'somethig': 799,\n",
              " 'someones': 800,\n",
              " 'soft': 801,\n",
              " 'sodomy': 802,\n",
              " 'socially': 803,\n",
              " 'social': 804,\n",
              " 'soaked': 805,\n",
              " 'smoking': 806,\n",
              " 'slightly': 807,\n",
              " 'slept': 808,\n",
              " 'sleeping': 809,\n",
              " 'slashes': 810,\n",
              " 'slammed': 811,\n",
              " 'sixty': 812,\n",
              " 'situation': 813,\n",
              " 'siting': 814,\n",
              " 'sit': 815,\n",
              " 'sisters': 816,\n",
              " 'sister': 817,\n",
              " 'single': 818,\n",
              " 'since': 819,\n",
              " 'simple': 820,\n",
              " 'similarly': 821,\n",
              " 'silent': 822,\n",
              " 'sidewalk': 823,\n",
              " 'shrugged': 824,\n",
              " 'shown': 825,\n",
              " 'shouldn’t': 826,\n",
              " 'should': 827,\n",
              " 'shoudnt': 828,\n",
              " 'shotgun': 829,\n",
              " 'shorts': 830,\n",
              " 'shortly': 831,\n",
              " 'shock': 832,\n",
              " 'shirts': 833,\n",
              " 'shirt': 834,\n",
              " 'sheep': 835,\n",
              " 'shed': 836,\n",
              " 'share': 837,\n",
              " 'sexual': 838,\n",
              " 'set': 839,\n",
              " 'serving': 840,\n",
              " 'served': 841,\n",
              " 'serology': 842,\n",
              " 'seriously': 843,\n",
              " 'separation': 844,\n",
              " 'separate': 845,\n",
              " 'send': 846,\n",
              " 'seen': 847,\n",
              " 'seems': 848,\n",
              " 'seeing': 849,\n",
              " 'seductive': 850,\n",
              " 'seduced': 851,\n",
              " 'secure': 852,\n",
              " 'search': 853,\n",
              " 'screen': 854,\n",
              " 'scientific': 855,\n",
              " 'sane': 856,\n",
              " 'safest': 857,\n",
              " 'safe': 858,\n",
              " 'runs': 859,\n",
              " 'running': 860,\n",
              " 'rummaging': 861,\n",
              " 'route': 862,\n",
              " 'rope': 863,\n",
              " 'rooms': 864,\n",
              " 'rocky': 865,\n",
              " 'rocking': 866,\n",
              " 'robery': 867,\n",
              " 'roadway': 868,\n",
              " 'ringing': 869,\n",
              " 'rights': 870,\n",
              " 'rid': 871,\n",
              " 'rich': 872,\n",
              " 'rest': 873,\n",
              " 'responsability': 874,\n",
              " 'residence': 875,\n",
              " 'required': 876,\n",
              " 'requesting': 877,\n",
              " 'reputation': 878,\n",
              " 'represented': 879,\n",
              " 'representatives': 880,\n",
              " 'reporter': 881,\n",
              " 'reply': 882,\n",
              " 'rephrase': 883,\n",
              " 'repeatedly': 884,\n",
              " 'repeat': 885,\n",
              " 'relevance': 886,\n",
              " 'release': 887,\n",
              " 'relation': 888,\n",
              " 'related': 889,\n",
              " 'reginald': 890,\n",
              " 'reforming': 891,\n",
              " 'recover': 892,\n",
              " 'recording': 893,\n",
              " 'recollection': 894,\n",
              " 'recognized': 895,\n",
              " 'receiving': 896,\n",
              " 'receipt': 897,\n",
              " 'reason': 898,\n",
              " 'ready': 899,\n",
              " 'reading': 900,\n",
              " 'read': 901,\n",
              " 'reached': 902,\n",
              " 'rather': 903,\n",
              " 'ramon': 904,\n",
              " 'rag': 905,\n",
              " 'quitting': 906,\n",
              " 'quite': 907,\n",
              " 'quick': 908,\n",
              " 'purpose': 909,\n",
              " 'purdue': 910,\n",
              " 'pulled': 911,\n",
              " 'psychologists': 912,\n",
              " 'pry': 913,\n",
              " 'prove': 914,\n",
              " 'protection': 915,\n",
              " 'protecting': 916,\n",
              " 'prosecutor': 917,\n",
              " 'property': 918,\n",
              " 'processing': 919,\n",
              " 'proceeded': 920,\n",
              " 'prob': 921,\n",
              " 'prisioners': 922,\n",
              " 'primary': 923,\n",
              " 'primarily': 924,\n",
              " 'pretending': 925,\n",
              " 'presume': 926,\n",
              " 'presumably': 927,\n",
              " 'pressure': 928,\n",
              " 'prepared': 929,\n",
              " 'predator': 930,\n",
              " 'preceded': 931,\n",
              " 'possibly': 932,\n",
              " 'possible': 933,\n",
              " 'possibility': 934,\n",
              " 'portion': 935,\n",
              " 'pointed': 936,\n",
              " 'placelooking': 937,\n",
              " 'pistols': 938,\n",
              " 'pipe': 939,\n",
              " 'pillow': 940,\n",
              " 'pieces': 941,\n",
              " 'picked': 942,\n",
              " 'physically': 943,\n",
              " 'physical': 944,\n",
              " 'persons': 945,\n",
              " 'personally': 946,\n",
              " 'personality': 947,\n",
              " 'personal': 948,\n",
              " 'persecute': 949,\n",
              " 'perpetrators': 950,\n",
              " 'permanency': 951,\n",
              " 'perimeter': 952,\n",
              " 'perez': 953,\n",
              " 'pause': 954,\n",
              " 'passenger': 955,\n",
              " 'passed': 956,\n",
              " 'particular': 957,\n",
              " 'partially': 958,\n",
              " 'part': 959,\n",
              " 'parole': 960,\n",
              " 'parents': 961,\n",
              " 'paper': 962,\n",
              " 'pain': 963,\n",
              " 'outcome': 964,\n",
              " 'ourselves': 965,\n",
              " 'ordered': 966,\n",
              " 'opened': 967,\n",
              " 'onto': 968,\n",
              " 'one”': 969,\n",
              " 'ones': 970,\n",
              " 'oneonone': 971,\n",
              " 'omi': 972,\n",
              " 'oldest': 973,\n",
              " 'often': 974,\n",
              " 'offender': 975,\n",
              " 'obviously': 976,\n",
              " 'obstacle': 977,\n",
              " 'o': 978,\n",
              " 'nursery': 979,\n",
              " 'nov': 980,\n",
              " 'notice': 981,\n",
              " 'note': 982,\n",
              " 'normal': 983,\n",
              " 'nor': 984,\n",
              " 'nolan': 985,\n",
              " 'noise': 986,\n",
              " 'ninetynine': 987,\n",
              " 'nineteenth': 988,\n",
              " 'nepolian': 989,\n",
              " 'nephew': 990,\n",
              " 'negative': 991,\n",
              " 'needs': 992,\n",
              " 'necessary': 993,\n",
              " 'necessarily': 994,\n",
              " 'near': 995,\n",
              " 'national': 996,\n",
              " 'nancy': 997,\n",
              " 'names': 998,\n",
              " 'murderers': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z9pgJK-sksP"
      },
      "source": [
        "Above **make_vector** function is used to convert given transcript( sentence) to a vector containing an index of each word and trimmed or padded to lenth __m__ = __100__. \n",
        "\n",
        "**Example:**\n",
        "\n",
        "Consider this transcript:\n",
        "\n",
        "    \"No sir I was not, not at all.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jW0pdDosksP",
        "outputId": "2d12acb3-2bf9-4912-8c90-c03e88103395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence: No sir I was not, not at all.\n",
            "word2vector:\n",
            " tf.Tensor(\n",
            "[[ 60 261   2   6  31  31  27  45   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]], shape=(1, 100), dtype=int64) 0\n"
          ]
        }
      ],
      "source": [
        "for i,sent in enumerate(AllData[:,0]):\n",
        "    if len(sent.split()) == 8:\n",
        "        print(\"Sentence:\",sent )\n",
        "        prinot(\"word2vector:\\n\",make_vector([sent]),AllData[i,1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBrgt-_KsksP"
      },
      "source": [
        "Above result shows the conversion of that given sentence using one-hot encoding.\n",
        "\n",
        "In that result 60, 261,   2,   6,  31,  31,  27,  and 45 shows index of word \"No\", \"sir\", \"I\", \"was\", \"not\", \"at\" and \"all\" in the vocabulary. rest is 0 as there are only 8 words in a sentence and we need vector with lenght of 100.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MjI8NtpsksP"
      },
      "source": [
        "## Using GloVe\n",
        "\n",
        "For converting a **word** in to a __n__ dimentional vector a converison matrix is need to be created for our vocabulary from the **GloVe** dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMT3jS-4sksP",
        "outputId": "9447c3fe-afd5-409c-947c-16886efc97b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Total word found in GloVe: 1468  out of: 1521\n"
          ]
        }
      ],
      "source": [
        "# Now creatnig a matrix from the vectors of words\n",
        "total_tokens = len(vocab_idx) + 2  # Tokens is the indvidual words from vocabulary\n",
        "dimention_vect = 300  # Dimention of a vector of GloVe dataset\n",
        "l = 0\n",
        "conversion_matrix = np.zeros((total_tokens,dimention_vect)) # Created matrix\n",
        "nf_words = []\n",
        "hit,miss = (0,0)\n",
        "for word,i in vocab_idx.items():\n",
        "    \n",
        "    if word in embodided_word_map.keys():\n",
        "        ''' Check if that word present in GloVe dataset'''\n",
        "        conversion_matrix[i] = embodided_word_map[word]\n",
        "        hit = hit +1\n",
        "    else:\n",
        "        nf_words.append(word)\n",
        "        miss = miss + 1\n",
        "\n",
        "print(\" Total word found in GloVe:\",hit,\" out of:\",hit+miss)\n",
        "# embodided_word_map = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDv3-4BJsksQ",
        "outputId": "bd92ada1-5e9c-4d2c-a13e-c436e0ce0e71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1523, 300)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversion_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MshC2OfsksQ"
      },
      "source": [
        "Here Some of the words were not available in the GloVe so for that we are just considering the all 0 vectors. As this number is small so it should not impact the result.\n",
        "\n",
        "To convert given word in to a 300-dimentional vector that \"conversion_matrix\" is used.\n",
        "\n",
        "For example:\n",
        "    \n",
        "Word \"No\" is represented by $60^{th}$ row of \"conversion_matrix\" since index of word \"No\" in vocabulary is 60.\n",
        "Similary word \"sir\" is represented by $261^{th}$ row of \"conversion_matrix\"..\n",
        "\n",
        "So using that a simple one-hote encoded vector of transcript(sentence) is converted to __n__ x __m__ sized matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DIN0cPJsksQ"
      },
      "outputs": [],
      "source": [
        "# For converting text to 300x100 vector will be using Embedding layer from keras\n",
        "embedding_layer = Embedding(\n",
        "    total_tokens,\n",
        "    dimention_vect,\n",
        "    embeddings_initializer=keras.initializers.Constant(conversion_matrix),\n",
        "    trainable=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w6CoTRxsksQ"
      },
      "source": [
        "# Preparing Training and Testing Data\n",
        "\n",
        "Since, the data set whcihc we have created is a matrix with 2 columns and first half is of one category and \n",
        "another half is of another. Then each input is converted to 300x100 matrix using the embedding_layer.\n",
        "\n",
        "And then First 70% of dataset is used as traning and remaining is used as testing.\n",
        "\n",
        "Input and labels are seperated also."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rk3BKB31sksQ"
      },
      "source": [
        "## Split data in training and testing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6fCgUkfsksQ"
      },
      "outputs": [],
      "source": [
        "np.random.shuffle(AllData)  # Shuffeling data row wise\n",
        "x = make_vector(np.array([[s] for s in AllData[:,0]])).numpy() # Creating word vector for all the transcripts\n",
        "\n",
        "data_in = []\n",
        "for s in x:\n",
        "    data_in.append(embedding_layer(s).numpy().T)\n",
        "data_in = np.array(data_in)   \n",
        "\n",
        "# Now spliting data in training and testing\n",
        "n = int(0.7 * x.shape[0])\n",
        "x_train = data_in[:n]\n",
        "y_train = np.array(AllData[:n,1],dtype='float32')\n",
        "\n",
        "x_test = data_in[n:]\n",
        "y_test = np.array(AllData[n:,1],dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvB41GQfsksQ"
      },
      "source": [
        "# Build model and train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSfJjxTusksQ",
        "outputId": "25804907-227f-4030-e0b2-b7b85f5841ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 300, 100)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 293, 3)            2403      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 146, 3)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 146, 3)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 438)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 300)               131700    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              308224    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 444,377\n",
            "Trainable params: 444,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\",\n",
        "    min_delta=0.01,\n",
        "    patience=110,\n",
        "    verbose=0,\n",
        "    mode=\"max\",\n",
        "    baseline=None,\n",
        "    restore_best_weights=True)\n",
        "\n",
        "op_m = keras.optimizers.RMSprop(\n",
        "    learning_rate=0.001, rho=0.9, momentum=0.001, epsilon=1e-07, centered=False,\n",
        "    name='RMSprop'\n",
        ")\n",
        "\n",
        "int_sequences_input = keras.Input(shape=(300,100))\n",
        "\n",
        "x = layers.Conv1D(3,8,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.001)\n",
        "                 )(int_sequences_input)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "# x = layers.Conv1D(2,15,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
        "# x = layers.MaxPooling1D(2)(x)\n",
        "# # x = layers.Dropout(0.2)(x)\n",
        "\n",
        "# x = layers.Conv1D(2,15,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
        "# x = layers.MaxPooling1D(2)(x)\n",
        "# # x = layers.Dropout(0.2)(x)\n",
        "\n",
        "# x = layers.Conv1D(2,15,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
        "# x = layers.MaxPooling1D(2)(x)\n",
        "# x = layers.Dropout(0.2)(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "\n",
        "x = layers.Dense(300,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.001))(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Dense(1024,activation=\"relu\",kernel_regularizer=keras.regularizers.l2(0.01))(x)\n",
        "x = layers.Dropout(0.6)(x)\n",
        "\n",
        "# x = layers.Dense(50,activation=\"relu\")(x)\n",
        "# x = layers.Dropout(0.2)(x)\n",
        "\n",
        "preds = layers.Dense(2, activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0xsnnRSfsksS",
        "outputId": "103abc52-6de0-4b01-ece2-d01ab5608276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "3/3 [==============================] - 0s 64ms/step - loss: 5.5993 - acc: 0.5119 - val_loss: 4.9078 - val_acc: 0.4595\n",
            "Epoch 2/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 4.7229 - acc: 0.5714 - val_loss: 4.4472 - val_acc: 0.4865\n",
            "Epoch 3/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 4.3705 - acc: 0.5119 - val_loss: 4.1351 - val_acc: 0.5405\n",
            "Epoch 4/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 3.9952 - acc: 0.5714 - val_loss: 3.9731 - val_acc: 0.4595\n",
            "Epoch 5/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.7667 - acc: 0.5595 - val_loss: 3.6212 - val_acc: 0.4595\n",
            "Epoch 6/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.4669 - acc: 0.6071 - val_loss: 3.4248 - val_acc: 0.4054\n",
            "Epoch 7/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 3.1649 - acc: 0.7976 - val_loss: 3.2643 - val_acc: 0.4865\n",
            "Epoch 8/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 3.0093 - acc: 0.7143 - val_loss: 3.1506 - val_acc: 0.4595\n",
            "Epoch 9/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.8605 - acc: 0.7381 - val_loss: 2.9759 - val_acc: 0.3784\n",
            "Epoch 10/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 2.6655 - acc: 0.7738 - val_loss: 2.8431 - val_acc: 0.3784\n",
            "Epoch 11/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 2.4527 - acc: 0.8333 - val_loss: 2.7389 - val_acc: 0.5135\n",
            "Epoch 12/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 2.7167 - acc: 0.5833 - val_loss: 2.5437 - val_acc: 0.5135\n",
            "Epoch 13/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 2.2060 - acc: 0.8214 - val_loss: 2.4881 - val_acc: 0.4324\n",
            "Epoch 14/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 2.0801 - acc: 0.8690 - val_loss: 2.3656 - val_acc: 0.4324\n",
            "Epoch 15/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.9292 - acc: 0.8452 - val_loss: 2.4094 - val_acc: 0.4865\n",
            "Epoch 16/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.8578 - acc: 0.7976 - val_loss: 2.3318 - val_acc: 0.5135\n",
            "Epoch 17/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.6573 - acc: 0.9048 - val_loss: 2.1885 - val_acc: 0.4595\n",
            "Epoch 18/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.5534 - acc: 0.9048 - val_loss: 2.0467 - val_acc: 0.5405\n",
            "Epoch 19/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 1.4524 - acc: 0.9167 - val_loss: 2.0580 - val_acc: 0.4324\n",
            "Epoch 20/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 1.5284 - acc: 0.8214 - val_loss: 1.9429 - val_acc: 0.4865\n",
            "Epoch 21/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.2550 - acc: 0.9524 - val_loss: 2.2209 - val_acc: 0.4324\n",
            "Epoch 22/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.2671 - acc: 0.9167 - val_loss: 1.8907 - val_acc: 0.5676\n",
            "Epoch 23/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 1.2460 - acc: 0.8690 - val_loss: 2.5465 - val_acc: 0.4324\n",
            "Epoch 24/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 1.1639 - acc: 0.9286 - val_loss: 1.7993 - val_acc: 0.4865\n",
            "Epoch 25/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 1.0522 - acc: 0.9762 - val_loss: 1.8211 - val_acc: 0.4595\n",
            "Epoch 26/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 1.1289 - acc: 0.8571 - val_loss: 1.8278 - val_acc: 0.5946\n",
            "Epoch 27/1000\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.9311 - acc: 0.9643 - val_loss: 1.7657 - val_acc: 0.4595\n",
            "Epoch 28/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.9106 - acc: 0.9643 - val_loss: 2.0591 - val_acc: 0.4324\n",
            "Epoch 29/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.8432 - acc: 0.9762 - val_loss: 1.7895 - val_acc: 0.4595\n",
            "Epoch 30/1000\n",
            "3/3 [==============================] - 0s 30ms/step - loss: 0.8592 - acc: 0.9405 - val_loss: 1.8124 - val_acc: 0.4595\n",
            "Epoch 31/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.8858 - acc: 0.9167 - val_loss: 1.7947 - val_acc: 0.5135\n",
            "Epoch 32/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.7563 - acc: 0.9762 - val_loss: 1.7160 - val_acc: 0.4324\n",
            "Epoch 33/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.7413 - acc: 0.9643 - val_loss: 1.6526 - val_acc: 0.5946\n",
            "Epoch 34/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.7743 - acc: 0.9286 - val_loss: 1.6533 - val_acc: 0.4324\n",
            "Epoch 35/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.7166 - acc: 0.9524 - val_loss: 2.0560 - val_acc: 0.4324\n",
            "Epoch 36/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6669 - acc: 0.9524 - val_loss: 1.7037 - val_acc: 0.4324\n",
            "Epoch 37/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.6529 - acc: 0.9643 - val_loss: 2.0177 - val_acc: 0.5405\n",
            "Epoch 38/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.5852 - acc: 0.9762 - val_loss: 1.6046 - val_acc: 0.5405\n",
            "Epoch 39/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.6395 - acc: 0.9286 - val_loss: 2.0203 - val_acc: 0.4324\n",
            "Epoch 40/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5337 - acc: 0.9881 - val_loss: 1.5966 - val_acc: 0.5135\n",
            "Epoch 41/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.4992 - acc: 1.0000 - val_loss: 1.6823 - val_acc: 0.4324\n",
            "Epoch 42/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5385 - acc: 0.9643 - val_loss: 1.6183 - val_acc: 0.5676\n",
            "Epoch 43/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.5258 - acc: 0.9643 - val_loss: 1.7465 - val_acc: 0.5946\n",
            "Epoch 44/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4854 - acc: 0.9762 - val_loss: 1.6847 - val_acc: 0.4595\n",
            "Epoch 45/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.4464 - acc: 0.9881 - val_loss: 1.6424 - val_acc: 0.5676\n",
            "Epoch 46/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6084 - acc: 0.9048 - val_loss: 2.2646 - val_acc: 0.4324\n",
            "Epoch 47/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4548 - acc: 0.9762 - val_loss: 1.6158 - val_acc: 0.4865\n",
            "Epoch 48/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.4034 - acc: 1.0000 - val_loss: 1.7306 - val_acc: 0.4324\n",
            "Epoch 49/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.4043 - acc: 0.9881 - val_loss: 1.7680 - val_acc: 0.3784\n",
            "Epoch 50/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3774 - acc: 1.0000 - val_loss: 2.1355 - val_acc: 0.4324\n",
            "Epoch 51/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.5204 - acc: 0.9048 - val_loss: 1.8653 - val_acc: 0.4054\n",
            "Epoch 52/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.3894 - acc: 0.9881 - val_loss: 1.6030 - val_acc: 0.5676\n",
            "Epoch 53/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.3501 - acc: 1.0000 - val_loss: 1.7722 - val_acc: 0.4054\n",
            "Epoch 54/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3552 - acc: 0.9881 - val_loss: 1.6489 - val_acc: 0.5946\n",
            "Epoch 55/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.3365 - acc: 1.0000 - val_loss: 1.7070 - val_acc: 0.4595\n",
            "Epoch 56/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.3123 - acc: 1.0000 - val_loss: 1.6790 - val_acc: 0.4595\n",
            "Epoch 57/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.3137 - acc: 0.9881 - val_loss: 2.5871 - val_acc: 0.5405\n",
            "Epoch 58/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.3770 - acc: 0.9524 - val_loss: 2.0376 - val_acc: 0.4324\n",
            "Epoch 59/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.3121 - acc: 0.9881 - val_loss: 1.7771 - val_acc: 0.5676\n",
            "Epoch 60/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2994 - acc: 0.9881 - val_loss: 1.5880 - val_acc: 0.4865\n",
            "Epoch 61/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3305 - acc: 0.9762 - val_loss: 1.8296 - val_acc: 0.3784\n",
            "Epoch 62/1000\n",
            "3/3 [==============================] - 0s 29ms/step - loss: 0.3021 - acc: 0.9762 - val_loss: 1.7256 - val_acc: 0.4865\n",
            "Epoch 63/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.3151 - acc: 0.9643 - val_loss: 1.8372 - val_acc: 0.4324\n",
            "Epoch 64/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2761 - acc: 1.0000 - val_loss: 1.8108 - val_acc: 0.4054\n",
            "Epoch 65/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2607 - acc: 1.0000 - val_loss: 1.7057 - val_acc: 0.5676\n",
            "Epoch 66/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2581 - acc: 1.0000 - val_loss: 2.5832 - val_acc: 0.4324\n",
            "Epoch 67/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.3160 - acc: 0.9643 - val_loss: 2.1409 - val_acc: 0.4054\n",
            "Epoch 68/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2523 - acc: 0.9881 - val_loss: 1.6373 - val_acc: 0.5676\n",
            "Epoch 69/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2329 - acc: 1.0000 - val_loss: 1.7267 - val_acc: 0.4324\n",
            "Epoch 70/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.2330 - acc: 1.0000 - val_loss: 1.8525 - val_acc: 0.4054\n",
            "Epoch 71/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2287 - acc: 1.0000 - val_loss: 1.9219 - val_acc: 0.4324\n",
            "Epoch 72/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2373 - acc: 0.9881 - val_loss: 1.8088 - val_acc: 0.4865\n",
            "Epoch 73/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2183 - acc: 1.0000 - val_loss: 1.7110 - val_acc: 0.5405\n",
            "Epoch 74/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2109 - acc: 1.0000 - val_loss: 2.1035 - val_acc: 0.5135\n",
            "Epoch 75/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4080 - acc: 0.9167 - val_loss: 1.5384 - val_acc: 0.5135\n",
            "Epoch 76/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2537 - acc: 0.9881 - val_loss: 1.5234 - val_acc: 0.5135\n",
            "Epoch 77/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2199 - acc: 1.0000 - val_loss: 1.5799 - val_acc: 0.5405\n",
            "Epoch 78/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2115 - acc: 0.9881 - val_loss: 1.6442 - val_acc: 0.4595\n",
            "Epoch 79/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1902 - acc: 1.0000 - val_loss: 1.6089 - val_acc: 0.4865\n",
            "Epoch 80/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1973 - acc: 1.0000 - val_loss: 1.6102 - val_acc: 0.5405\n",
            "Epoch 81/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1832 - acc: 1.0000 - val_loss: 1.5741 - val_acc: 0.5405\n",
            "Epoch 82/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1880 - acc: 1.0000 - val_loss: 1.6130 - val_acc: 0.4865\n",
            "Epoch 83/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.2537 - acc: 0.9762 - val_loss: 1.5448 - val_acc: 0.5135\n",
            "Epoch 84/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2862 - acc: 0.9643 - val_loss: 1.4887 - val_acc: 0.5405\n",
            "Epoch 85/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1860 - acc: 1.0000 - val_loss: 2.3846 - val_acc: 0.4324\n",
            "Epoch 86/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2746 - acc: 0.9762 - val_loss: 1.5515 - val_acc: 0.5405\n",
            "Epoch 87/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1712 - acc: 1.0000 - val_loss: 1.5267 - val_acc: 0.5405\n",
            "Epoch 88/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2127 - acc: 0.9762 - val_loss: 1.5702 - val_acc: 0.5135\n",
            "Epoch 89/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.2122 - acc: 0.9762 - val_loss: 1.7038 - val_acc: 0.4595\n",
            "Epoch 90/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1676 - acc: 1.0000 - val_loss: 1.6759 - val_acc: 0.4865\n",
            "Epoch 91/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1658 - acc: 1.0000 - val_loss: 1.8470 - val_acc: 0.4865\n",
            "Epoch 92/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1693 - acc: 1.0000 - val_loss: 1.6754 - val_acc: 0.4595\n",
            "Epoch 93/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.3148 - acc: 0.9286 - val_loss: 2.3868 - val_acc: 0.4324\n",
            "Epoch 94/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1735 - acc: 1.0000 - val_loss: 1.8832 - val_acc: 0.4595\n",
            "Epoch 95/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1604 - acc: 1.0000 - val_loss: 1.6686 - val_acc: 0.4595\n",
            "Epoch 96/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1720 - acc: 0.9881 - val_loss: 1.6846 - val_acc: 0.4865\n",
            "Epoch 97/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1530 - acc: 1.0000 - val_loss: 1.5915 - val_acc: 0.5135\n",
            "Epoch 98/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1677 - acc: 0.9881 - val_loss: 1.5105 - val_acc: 0.5946\n",
            "Epoch 99/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1545 - acc: 1.0000 - val_loss: 1.6070 - val_acc: 0.4865\n",
            "Epoch 100/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1746 - acc: 0.9881 - val_loss: 1.7980 - val_acc: 0.4324\n",
            "Epoch 101/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1599 - acc: 1.0000 - val_loss: 1.6647 - val_acc: 0.4324\n",
            "Epoch 102/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1584 - acc: 0.9881 - val_loss: 2.4363 - val_acc: 0.4324\n",
            "Epoch 103/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1439 - acc: 1.0000 - val_loss: 1.8423 - val_acc: 0.4324\n",
            "Epoch 104/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1481 - acc: 1.0000 - val_loss: 1.9243 - val_acc: 0.4054\n",
            "Epoch 105/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1750 - acc: 0.9881 - val_loss: 1.5279 - val_acc: 0.4865\n",
            "Epoch 106/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1515 - acc: 1.0000 - val_loss: 1.5383 - val_acc: 0.5405\n",
            "Epoch 107/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1335 - acc: 1.0000 - val_loss: 1.8461 - val_acc: 0.4595\n",
            "Epoch 108/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1735 - acc: 0.9762 - val_loss: 2.6265 - val_acc: 0.4324\n",
            "Epoch 109/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1805 - acc: 0.9643 - val_loss: 1.6860 - val_acc: 0.5135\n",
            "Epoch 110/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1347 - acc: 1.0000 - val_loss: 1.6624 - val_acc: 0.4865\n",
            "Epoch 111/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1396 - acc: 1.0000 - val_loss: 1.8360 - val_acc: 0.4324\n",
            "Epoch 112/1000\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1967 - acc: 0.9643 - val_loss: 1.7787 - val_acc: 0.4865\n",
            "Epoch 113/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1287 - acc: 1.0000 - val_loss: 2.0302 - val_acc: 0.5135\n",
            "Epoch 114/1000\n",
            "3/3 [==============================] - 0s 27ms/step - loss: 0.1291 - acc: 1.0000 - val_loss: 1.8152 - val_acc: 0.4865\n",
            "Epoch 115/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1260 - acc: 1.0000 - val_loss: 1.6667 - val_acc: 0.5135\n",
            "Epoch 116/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1347 - acc: 0.9881 - val_loss: 2.3956 - val_acc: 0.4324\n",
            "Epoch 117/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1238 - acc: 1.0000 - val_loss: 1.6329 - val_acc: 0.4865\n",
            "Epoch 118/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1314 - acc: 1.0000 - val_loss: 2.0261 - val_acc: 0.4595\n",
            "Epoch 119/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1306 - acc: 1.0000 - val_loss: 1.6466 - val_acc: 0.5405\n",
            "Epoch 120/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1177 - acc: 1.0000 - val_loss: 1.8312 - val_acc: 0.5135\n",
            "Epoch 121/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1175 - acc: 1.0000 - val_loss: 2.3003 - val_acc: 0.4324\n",
            "Epoch 122/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1180 - acc: 1.0000 - val_loss: 1.6308 - val_acc: 0.5405\n",
            "Epoch 123/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1110 - acc: 1.0000 - val_loss: 2.0529 - val_acc: 0.4865\n",
            "Epoch 124/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1123 - acc: 1.0000 - val_loss: 1.7434 - val_acc: 0.5135\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 125/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1059 - acc: 1.0000 - val_loss: 1.7875 - val_acc: 0.5135\n",
            "Epoch 126/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.1077 - acc: 1.0000 - val_loss: 1.6160 - val_acc: 0.5405\n",
            "Epoch 127/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1473 - acc: 0.9881 - val_loss: 1.7826 - val_acc: 0.5135\n",
            "Epoch 128/1000\n",
            "3/3 [==============================] - 0s 28ms/step - loss: 0.1070 - acc: 1.0000 - val_loss: 1.7462 - val_acc: 0.5135\n",
            "Epoch 129/1000\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1009 - acc: 1.0000 - val_loss: 1.7004 - val_acc: 0.5135\n",
            "Epoch 130/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1078 - acc: 1.0000 - val_loss: 1.8532 - val_acc: 0.4324\n",
            "Epoch 131/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0997 - acc: 1.0000 - val_loss: 1.6074 - val_acc: 0.5405\n",
            "Epoch 132/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.1450 - acc: 0.9881 - val_loss: 3.1432 - val_acc: 0.4324\n",
            "Epoch 133/1000\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.1215 - acc: 0.9881 - val_loss: 1.5242 - val_acc: 0.5405\n",
            "Epoch 134/1000\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.0935 - acc: 1.0000 - val_loss: 1.5973 - val_acc: 0.5135\n",
            "Epoch 135/1000\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.0925 - acc: 1.0000 - val_loss: 1.5987 - val_acc: 0.5135\n",
            "Epoch 136/1000\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.0907 - acc: 1.0000 - val_loss: 1.6589 - val_acc: 0.5405\n"
          ]
        }
      ],
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\", optimizer='rmsprop', metrics=[\"acc\"]\n",
        ")\n",
        "history = model.fit(x_train, y_train,epochs=1000,validation_data=(x_test, y_test),callbacks=[cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrqRwPE_sksS",
        "outputId": "ad5de8fa-3849-4298-f948-8d5f1d23e492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Test Data: 0.5945945945945946\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(model.predict(x_test),axis=1)\n",
        "print(\"Accuracy on Test Data:\", np.sum(pred == y_test)/y_test.shape[0])"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "r5L7tHSmsksS"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtR5bOYlsksS"
      },
      "outputs": [],
      "source": [
        "test_model = models.load_model(\"BestSofar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YdgrWGPsksS",
        "outputId": "fbc052e6-6a7b-4ba1-a6c9-7d1b2cda3470"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 300, 100)]        0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 293, 3)            2403      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 146, 3)            0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 146, 3)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 438)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 300)               131700    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1024)              308224    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 2)                 2050      \n",
            "=================================================================\n",
            "Total params: 444,377\n",
            "Trainable params: 444,377\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "dIGPQettsksS",
        "outputId": "2c29b689-f7a7-4723-e394-75c1dd780715"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on Test Data: 0.8648648648648649\n"
          ]
        }
      ],
      "source": [
        "pred = np.argmax(test_model.predict(x_test),axis=1)\n",
        "print(\"Accuracy on Test Data:\", np.sum(pred == y_test)/y_test.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHlzb4HHsksT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}